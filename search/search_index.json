{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Motivation With Panda like dataframe, do you want to create a code, and choose at the end, the framework to use? Do you want to be able to choose the best framework after simply performing performance measurements? This framework unifies multiple Panda-compatible components, to allow the writing of a single code, compatible with all. Synopsis With some parameters and Virtual classes, it's possible to write a code, and execute this code: With or without multicore With or without cluster (multi nodes) With or without GPU To do that, we create some virtual classes, add some methods in others classes, etc. To reduce the confusion, you must use the classes VDataFrame and VSeries (The prefix V is for Virtual ). These classes propose the methods .to_pandas() and .compute() for each version, but are the real classes of the selected framework. A new @delayed annotation can be use, with or without Dask. With some parameters, the real classes may be pandas.DataFrame , modin.pandas.DataFrame , cudf.DataFrame , dask.dataframe.DataFrame with Pandas or dask.dataframe.DataFrame with cudf (with Pandas or cudf for each partition). To manage the initialisation of a Dask, you must use the VClient() . This alias, can be automatically initialized with some environment variables. # Sample of code, compatible Pandas, cudf, modin, dask and dask_cudf from virtual_dataframe import * TestDF = VDataFrame with (VClient()): @delayed def my_function(data: TestDF) -> TestDF: return data rc = my_function(VDataFrame({\"data\": [1, 2]}, npartitions=2)) print(rc.to_pandas()) With this framework, you can select your environment, to run or debug your code. env Environement VDF_MODE=pandas Only Python with classical pandas VDF_MODE=modin Python with local multiple process VDF_MODE=cudf Python with local cuDF (GPU) VDF_MODE=dask Dask with local multiple process and pandas VDF_MODE=dask_cudf Dask with local multiple process and cuDF VDF_MODE=dask DEBUG=True Dask with single thread and pandas VDF_MODE=dask_cudf DEBUG=True Dask with single thread and cuDF VDF_MODE=dask VDF_CLUSTER=dask://.local Dask with local cluster and pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://.local Dask with local cuda cluster and cuDF VDF_MODE=dask VDF_CLUSTER=dask://...:ppp Dask with remote cluster and Pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://...:ppp Dask with remote cluster and cuDF VDF_MODE=dask_modin VDF_CLUSTER=dask://.local Dask with local cluster and modin VDF_MODE=dask_modin VDF_CLUSTER=dask://...:ppp Dask with remote cluster and modin The real compatibilty between the differents simulation of Pandas, depends on the implement of the modin, cudf or dask. Sometime, you can use the VDF_MODE variable, to update some part of code, between the selected backend. It's not always easy to write a code compatible with all scenario, but it's possible. Generally, add just .compute() and/or .to_pandas() at the end of the ETL, is enough. But, you must use, only the common feature with all frameworks. After this effort, it's possible to compare the performance about the differents technologies, or propose a component, compatible with differents scenario. For the deployment of your project, you can select the best framework for your process (in a dockerfile?), with only one ou two environment variables.","title":"Home"},{"location":"#motivation","text":"With Panda like dataframe, do you want to create a code, and choose at the end, the framework to use? Do you want to be able to choose the best framework after simply performing performance measurements? This framework unifies multiple Panda-compatible components, to allow the writing of a single code, compatible with all.","title":"Motivation"},{"location":"#synopsis","text":"With some parameters and Virtual classes, it's possible to write a code, and execute this code: With or without multicore With or without cluster (multi nodes) With or without GPU To do that, we create some virtual classes, add some methods in others classes, etc. To reduce the confusion, you must use the classes VDataFrame and VSeries (The prefix V is for Virtual ). These classes propose the methods .to_pandas() and .compute() for each version, but are the real classes of the selected framework. A new @delayed annotation can be use, with or without Dask. With some parameters, the real classes may be pandas.DataFrame , modin.pandas.DataFrame , cudf.DataFrame , dask.dataframe.DataFrame with Pandas or dask.dataframe.DataFrame with cudf (with Pandas or cudf for each partition). To manage the initialisation of a Dask, you must use the VClient() . This alias, can be automatically initialized with some environment variables. # Sample of code, compatible Pandas, cudf, modin, dask and dask_cudf from virtual_dataframe import * TestDF = VDataFrame with (VClient()): @delayed def my_function(data: TestDF) -> TestDF: return data rc = my_function(VDataFrame({\"data\": [1, 2]}, npartitions=2)) print(rc.to_pandas()) With this framework, you can select your environment, to run or debug your code. env Environement VDF_MODE=pandas Only Python with classical pandas VDF_MODE=modin Python with local multiple process VDF_MODE=cudf Python with local cuDF (GPU) VDF_MODE=dask Dask with local multiple process and pandas VDF_MODE=dask_cudf Dask with local multiple process and cuDF VDF_MODE=dask DEBUG=True Dask with single thread and pandas VDF_MODE=dask_cudf DEBUG=True Dask with single thread and cuDF VDF_MODE=dask VDF_CLUSTER=dask://.local Dask with local cluster and pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://.local Dask with local cuda cluster and cuDF VDF_MODE=dask VDF_CLUSTER=dask://...:ppp Dask with remote cluster and Pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://...:ppp Dask with remote cluster and cuDF VDF_MODE=dask_modin VDF_CLUSTER=dask://.local Dask with local cluster and modin VDF_MODE=dask_modin VDF_CLUSTER=dask://...:ppp Dask with remote cluster and modin The real compatibilty between the differents simulation of Pandas, depends on the implement of the modin, cudf or dask. Sometime, you can use the VDF_MODE variable, to update some part of code, between the selected backend. It's not always easy to write a code compatible with all scenario, but it's possible. Generally, add just .compute() and/or .to_pandas() at the end of the ETL, is enough. But, you must use, only the common feature with all frameworks. After this effort, it's possible to compare the performance about the differents technologies, or propose a component, compatible with differents scenario. For the deployment of your project, you can select the best framework for your process (in a dockerfile?), with only one ou two environment variables.","title":"Synopsis"},{"location":"CHANGELOG/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] Added Pandas, cudf, modin, dask, dask_cudf and dask_modin frameworks Changed Deprecated Removed Fixed Security [0.0.1] - YYYY-MM-DD TODO: Update CHANGELOG.md","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"[Unreleased]"},{"location":"CHANGELOG/#added","text":"Pandas, cudf, modin, dask, dask_cudf and dask_modin frameworks","title":"Added"},{"location":"CHANGELOG/#changed","text":"","title":"Changed"},{"location":"CHANGELOG/#deprecated","text":"","title":"Deprecated"},{"location":"CHANGELOG/#removed","text":"","title":"Removed"},{"location":"CHANGELOG/#fixed","text":"","title":"Fixed"},{"location":"CHANGELOG/#security","text":"","title":"Security"},{"location":"CHANGELOG/#001-yyyy-mm-dd","text":"TODO: Update CHANGELOG.md","title":"[0.0.1] - YYYY-MM-DD"},{"location":"api/","text":"API api comments vdf.@delayed Delayed function (do nothing or dask.delayed) vdf.concat(...) Merge VDataFrame vdf.read_csv(...) Read VDataFrame from CSVs glob files vdf.read_excel(...) * Read VDataFrame from Excel glob files vdf.read_fwf(...) * Read VDataFrame from Fwf glob files vdf.read_hdf(...) * Read VDataFrame from HDFs glob files vdf.read_json(...) Read VDataFrame from Jsons glob files vdf.read_orc(...) Read VDataFrame from ORCs glob files vdf.read_parquet(...) Read VDataFrame from Parquets glob files vdf.read_sql_table(...) * Read VDataFrame from SQL vdf.from_pandas(pdf, npartitions=...) Create Virtual Dataframe from Pandas DataFrame vdf.from_backend(vdf, npartitions=...) Create Virtual Dataframe from backend dataframe vdf.compute([...]) Compute multiple @delayed functions VDataFrame(data, npartitions=...) Create DataFrame in memory (only for test) VSeries(data, npartitions=...) Create Series in memory (only for test) VLocalCluster(...) Create a dask Local (CUDA) Cluster BackEndDataFrame The class of dask backend dataframe BackEndSeries The class of dask backend series BackEnd The backend framework VDataFrame.compute() Compute the virtual dataframe VDataFrame.persist() Persist the dataframe in memory VDataFrame.repartition() Rebalance the dataframe VDataFrame.visualize() Create an image with the graph VDataFrame.to_pandas() Convert to pandas dataframe VDataFrame.to_csv() Save to glob files VDataFrame.to_excel() * Save to glob files VDataFrame.to_feather() * Save to glob files VDataFrame.to_hdf() * Save to glob files VDataFrame.to_json() Save to glob files VDataFrame.to_orc() Save to glob files VDataFrame.to_parquet() Save to glob files VDataFrame.to_sql() * Save to sql table VDataFrame.to_numpy() Convert to numpy array VDataFrame.to_backend() Convert to backend dataframe VDataFrame.categorize() Detect all categories VDataFrame.apply_rows() Apply rows, GPU template VDataFrame.map_partitions() Apply function for each parttions VSeries.compute() Compute the virtual series VSeries.persist() Persist the dataframe in memory VSeries.repartition() Rebalance the dataframe VSeries.visualize() Create an image with the graph VSeries.to_pandas() Convert to pandas series VSeries.to_backend() Convert to backend series VSeries.to_numpy() Convert to numpy array VClient(...) The connexion with the cluster VDF_MODE The current mode Mode The enumeration of differents mode * some frameworks do not implement it, see below You can read a sample notebook here for an exemple of all API. Each API propose a specific version for each framework. For example: the toPandas() with Panda, return self @delayed use the dask @delayed or do nothing, and apply the code when the function was called. In the first case, the function return a part of the graph. In the second case, the function return immediately the result. read_csv(\"file*.csv\") read each file in parallele with dask by each worker, and read sequencially each file, and combine each dataframe, with a framework without distribution (pandas, modin, cudf) save_csv(\"file*.csv\") write each file in parallele with dask, and write one file \"file.csv\" (without the star) with a framework without distribution (pandas, modin, cudf) ...","title":"API"},{"location":"api/#api","text":"api comments vdf.@delayed Delayed function (do nothing or dask.delayed) vdf.concat(...) Merge VDataFrame vdf.read_csv(...) Read VDataFrame from CSVs glob files vdf.read_excel(...) * Read VDataFrame from Excel glob files vdf.read_fwf(...) * Read VDataFrame from Fwf glob files vdf.read_hdf(...) * Read VDataFrame from HDFs glob files vdf.read_json(...) Read VDataFrame from Jsons glob files vdf.read_orc(...) Read VDataFrame from ORCs glob files vdf.read_parquet(...) Read VDataFrame from Parquets glob files vdf.read_sql_table(...) * Read VDataFrame from SQL vdf.from_pandas(pdf, npartitions=...) Create Virtual Dataframe from Pandas DataFrame vdf.from_backend(vdf, npartitions=...) Create Virtual Dataframe from backend dataframe vdf.compute([...]) Compute multiple @delayed functions VDataFrame(data, npartitions=...) Create DataFrame in memory (only for test) VSeries(data, npartitions=...) Create Series in memory (only for test) VLocalCluster(...) Create a dask Local (CUDA) Cluster BackEndDataFrame The class of dask backend dataframe BackEndSeries The class of dask backend series BackEnd The backend framework VDataFrame.compute() Compute the virtual dataframe VDataFrame.persist() Persist the dataframe in memory VDataFrame.repartition() Rebalance the dataframe VDataFrame.visualize() Create an image with the graph VDataFrame.to_pandas() Convert to pandas dataframe VDataFrame.to_csv() Save to glob files VDataFrame.to_excel() * Save to glob files VDataFrame.to_feather() * Save to glob files VDataFrame.to_hdf() * Save to glob files VDataFrame.to_json() Save to glob files VDataFrame.to_orc() Save to glob files VDataFrame.to_parquet() Save to glob files VDataFrame.to_sql() * Save to sql table VDataFrame.to_numpy() Convert to numpy array VDataFrame.to_backend() Convert to backend dataframe VDataFrame.categorize() Detect all categories VDataFrame.apply_rows() Apply rows, GPU template VDataFrame.map_partitions() Apply function for each parttions VSeries.compute() Compute the virtual series VSeries.persist() Persist the dataframe in memory VSeries.repartition() Rebalance the dataframe VSeries.visualize() Create an image with the graph VSeries.to_pandas() Convert to pandas series VSeries.to_backend() Convert to backend series VSeries.to_numpy() Convert to numpy array VClient(...) The connexion with the cluster VDF_MODE The current mode Mode The enumeration of differents mode * some frameworks do not implement it, see below You can read a sample notebook here for an exemple of all API. Each API propose a specific version for each framework. For example: the toPandas() with Panda, return self @delayed use the dask @delayed or do nothing, and apply the code when the function was called. In the first case, the function return a part of the graph. In the second case, the function return immediately the result. read_csv(\"file*.csv\") read each file in parallele with dask by each worker, and read sequencially each file, and combine each dataframe, with a framework without distribution (pandas, modin, cudf) save_csv(\"file*.csv\") write each file in parallele with dask, and write one file \"file.csv\" (without the star) with a framework without distribution (pandas, modin, cudf) ...","title":"API"},{"location":"best_practices/","text":"Best practices For write a code, optimized with all frameworks, you must use some best practices . Use read file It's not a good idea to use the constructor of VDataFrame or VSeries because, all the datas must be in memory in the driver. If the dataframe is big, an out of memory can happen. To partitionning the job, create multiple files, and use read_csv(\"filename*.csv\") , use a big file and the auto partitioning proposed by dask or other file format. Then, each worker can read directly a partition. The constructor for VDataFrame and VSeries are present, only to help to write some unit test, but may not be used in production. No loop It's not a good idea to iterate over row of DataFrame. Some framework are very slow with this approach. It's better to use apply() or apply_rows() to distribute the code in the cluster and GPU. The code used in apply , will be compiled to CPU or GPU, before using, with some frameworks.","title":"Best practices"},{"location":"best_practices/#best-practices","text":"For write a code, optimized with all frameworks, you must use some best practices .","title":"Best practices"},{"location":"best_practices/#use-read-file","text":"It's not a good idea to use the constructor of VDataFrame or VSeries because, all the datas must be in memory in the driver. If the dataframe is big, an out of memory can happen. To partitionning the job, create multiple files, and use read_csv(\"filename*.csv\") , use a big file and the auto partitioning proposed by dask or other file format. Then, each worker can read directly a partition. The constructor for VDataFrame and VSeries are present, only to help to write some unit test, but may not be used in production.","title":"Use read file"},{"location":"best_practices/#no-loop","text":"It's not a good idea to iterate over row of DataFrame. Some framework are very slow with this approach. It's better to use apply() or apply_rows() to distribute the code in the cluster and GPU. The code used in apply , will be compiled to CPU or GPU, before using, with some frameworks.","title":"No loop"},{"location":"cluster/","text":"Cluster To connect to a cluster, use VDF_CLUSTER with protocol, host and optionaly, the port. dask://locahost:8787 or alternativelly, use DASK_SCHEDULER_SERVICE_HOST and DASK_SCHEDULER_SERVICE_PORT VDF_MODE DEBUG VDF_CLUSTER Scheduler pandas - - No scheduler cudf - - No scheduler modin - - No scheduler dask Yes - synchronous dask No - thread dask No dask://threads thread dask No dask://processes processes dask No dask://.local LocalCluster dask_modin No - LocalCluster dask_modin No dask://.local LocalCluster dask_modin No dask://<host>:<port> Dask cluster dask_cudf No dask://.local LocalCUDACluster dask_cudf No dask://<host>:<port> Dask cluster The special host name , ends with .local can be use to start a LocalCluster ( or LocalCUDACluster ) when your program is started. An instance of local cluster is started and injected in the Client . Then, your code can not manage the local cluster. To update the parameters for the implicit Local(CUDA)Cluster , you can use the Dask config file . local: scheduler-port: 0 device_memory_limit: 5G or you can set some environment variables. export DASK_LOCAL__SCHEDULER_PORT=0 export DASK_LOCAL__DEVICE_MEMORY_LIMIT=5g Sample: from virtual_dataframe import VClient with (VClient()) # Now, use the scheduler If you want to manage the parameters of Local(CUDA)Cluster , use the alternative VLocalCluster . from virtual_dataframe import VClient,VLocalCluster with (VClient(VLocalCluster())) # Now, use the scheduler","title":"Cluster"},{"location":"cluster/#cluster","text":"To connect to a cluster, use VDF_CLUSTER with protocol, host and optionaly, the port. dask://locahost:8787 or alternativelly, use DASK_SCHEDULER_SERVICE_HOST and DASK_SCHEDULER_SERVICE_PORT VDF_MODE DEBUG VDF_CLUSTER Scheduler pandas - - No scheduler cudf - - No scheduler modin - - No scheduler dask Yes - synchronous dask No - thread dask No dask://threads thread dask No dask://processes processes dask No dask://.local LocalCluster dask_modin No - LocalCluster dask_modin No dask://.local LocalCluster dask_modin No dask://<host>:<port> Dask cluster dask_cudf No dask://.local LocalCUDACluster dask_cudf No dask://<host>:<port> Dask cluster The special host name , ends with .local can be use to start a LocalCluster ( or LocalCUDACluster ) when your program is started. An instance of local cluster is started and injected in the Client . Then, your code can not manage the local cluster. To update the parameters for the implicit Local(CUDA)Cluster , you can use the Dask config file . local: scheduler-port: 0 device_memory_limit: 5G or you can set some environment variables. export DASK_LOCAL__SCHEDULER_PORT=0 export DASK_LOCAL__DEVICE_MEMORY_LIMIT=5g Sample: from virtual_dataframe import VClient with (VClient()) # Now, use the scheduler If you want to manage the parameters of Local(CUDA)Cluster , use the alternative VLocalCluster . from virtual_dataframe import VClient,VLocalCluster with (VClient(VLocalCluster())) # Now, use the scheduler","title":"Cluster"},{"location":"commands/","text":"Make commands The Makefile contains the central entry points for common tasks related to this project. Commands make help will print all majors target make configure will prepare the environment (conda venv, kernel, ...) make run-% will invoke all script in lexical order from scripts/<% dir> make lint will lint the code make test will run all tests make typing will check the typing make validate will validate the version before commit make clean will clean current environment make docs will create and show a HTML documentation in 'build/' make dist will create a full wheel distribution Jupyter commands make notebook will start a jupyter notebook make remove-kernel will remove the project's kernel make nb-run-% will execute all notebooks make nb-convert will convert all notebooks to python make clean-notebooks will clean all datas in the notebooks Twine commands make check-twine will check the packaging before publication make test-twine will publish the package in test.pypi.org <https://test.pypi.org> _) make twine will publish the package in pypi.org <https://pypi.org> _) Conda commands make conda-build will build the conda package make conda-debug will build the package in debug mode make conda-convert will convert the package for all platform make conda-install will install the conda package make conda-purge will remove the build artefact make conda-create will create a test environment for each mode","title":"Commands"},{"location":"commands/#make-commands","text":"The Makefile contains the central entry points for common tasks related to this project.","title":"Make commands"},{"location":"commands/#commands","text":"make help will print all majors target make configure will prepare the environment (conda venv, kernel, ...) make run-% will invoke all script in lexical order from scripts/<% dir> make lint will lint the code make test will run all tests make typing will check the typing make validate will validate the version before commit make clean will clean current environment make docs will create and show a HTML documentation in 'build/' make dist will create a full wheel distribution","title":"Commands"},{"location":"commands/#jupyter-commands","text":"make notebook will start a jupyter notebook make remove-kernel will remove the project's kernel make nb-run-% will execute all notebooks make nb-convert will convert all notebooks to python make clean-notebooks will clean all datas in the notebooks","title":"Jupyter commands"},{"location":"commands/#twine-commands","text":"make check-twine will check the packaging before publication make test-twine will publish the package in test.pypi.org <https://test.pypi.org> _) make twine will publish the package in pypi.org <https://pypi.org> _)","title":"Twine commands"},{"location":"commands/#conda-commands","text":"make conda-build will build the conda package make conda-debug will build the package in debug mode make conda-convert will convert the package for all platform make conda-install will install the conda package make conda-purge will remove the build artefact make conda-create will create a test environment for each mode","title":"Conda commands"},{"location":"compatibility/","text":"Compatibility This project is just a wrapper. So, it inherits limitations and bugs from other projects. Sorry for that. Limitations pandas All data must be in DRAM modin Read this cudf All data must be in VRAM All data types in cuDF are nullable Iterating over a cuDF Series, DataFrame or Index is not supported. Join (or merge) and groupby operations in cuDF do not guarantee output ordering. The order of operations is not always deterministic Cudf does not support duplicate column names Cudf also supports .apply() it relies on Numba to JIT compile the UDF and execute it on the GPU .apply(result_type=...) not supported dask transpose() and MultiIndex are not implemented Column assignment doesn't support type list dask_cudf See cudf and dask. Categories with strings not implemented File format compatibility To be compatible with all framework, you must only use the common features. We accept some function to read or write files, but we write a warning if you use a function not compatible with others frameworks. read_... / to_... pandas cudf modin dask dask_modin dask_cudf vdf.read_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_excel \u2713 \u2713 VDataFrame.to_excel \u2713 \u2713 VSeries.to_excel \u2713 \u2713 vdf.read_feather \u2713 \u2713 \u2713 VDataFrame.to_feather \u2713 \u2713 \u2713 vdf.read_fwf \u2713 \u2713 \u2713 \u2713 VDataFrame.to_fwf vdf.read_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_sql_table \u2713 \u2713 \u2713 \u2713 VDataFrame.to_sql \u2713 \u2713 \u2713 \u2713 VSeries.to_sql \u2713 \u2713 \u2713 \u2713 Cross framework compatibility small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask or dask_modin Limits:++ GPU cudf Limits:++ dask_cudf Limits:+++ To develop, you can choose the level to be compatible with others frameworks. Each cell is strongly compatible with the upper left part. No need of GPU? If you don't need to use a GPU, then develop for dask and use mode in bold . small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask or dask_modin Limits:++ GPU cudf Limits:++ dask_cudf Limits:+++ You can ignore this API: VDataFrame.apply_rows() No need of big data? If you don't need to use big data, then develop for cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask or dask_modin Limits:++ GPU cudf Limits:++ dask_cudf Limits:+++ You can ignore these API: @delayed map_partitions() categorize() compute() npartitions=... Need all possibility? To be compatible with all modes, develop for dask_cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask or dask_modin Limits:++ GPU cudf Limits:++ dask_cudf Limits:+++ and accept all the limitations.","title":"Compatibility"},{"location":"compatibility/#compatibility","text":"This project is just a wrapper. So, it inherits limitations and bugs from other projects. Sorry for that. Limitations pandas All data must be in DRAM modin Read this cudf All data must be in VRAM All data types in cuDF are nullable Iterating over a cuDF Series, DataFrame or Index is not supported. Join (or merge) and groupby operations in cuDF do not guarantee output ordering. The order of operations is not always deterministic Cudf does not support duplicate column names Cudf also supports .apply() it relies on Numba to JIT compile the UDF and execute it on the GPU .apply(result_type=...) not supported dask transpose() and MultiIndex are not implemented Column assignment doesn't support type list dask_cudf See cudf and dask. Categories with strings not implemented","title":"Compatibility"},{"location":"compatibility/#file-format-compatibility","text":"To be compatible with all framework, you must only use the common features. We accept some function to read or write files, but we write a warning if you use a function not compatible with others frameworks. read_... / to_... pandas cudf modin dask dask_modin dask_cudf vdf.read_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_excel \u2713 \u2713 VDataFrame.to_excel \u2713 \u2713 VSeries.to_excel \u2713 \u2713 vdf.read_feather \u2713 \u2713 \u2713 VDataFrame.to_feather \u2713 \u2713 \u2713 vdf.read_fwf \u2713 \u2713 \u2713 \u2713 VDataFrame.to_fwf vdf.read_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_sql_table \u2713 \u2713 \u2713 \u2713 VDataFrame.to_sql \u2713 \u2713 \u2713 \u2713 VSeries.to_sql \u2713 \u2713 \u2713 \u2713","title":"File format compatibility"},{"location":"compatibility/#cross-framework-compatibility","text":"small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask or dask_modin Limits:++ GPU cudf Limits:++ dask_cudf Limits:+++ To develop, you can choose the level to be compatible with others frameworks. Each cell is strongly compatible with the upper left part.","title":"Cross framework compatibility"},{"location":"compatibility/#no-need-of-gpu","text":"If you don't need to use a GPU, then develop for dask and use mode in bold . small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask or dask_modin Limits:++ GPU cudf Limits:++ dask_cudf Limits:+++ You can ignore this API: VDataFrame.apply_rows()","title":"No need of GPU?"},{"location":"compatibility/#no-need-of-big-data","text":"If you don't need to use big data, then develop for cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask or dask_modin Limits:++ GPU cudf Limits:++ dask_cudf Limits:+++ You can ignore these API: @delayed map_partitions() categorize() compute() npartitions=...","title":"No need of big data?"},{"location":"compatibility/#need-all-possibility","text":"To be compatible with all modes, develop for dask_cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask or dask_modin Limits:++ GPU cudf Limits:++ dask_cudf Limits:+++ and accept all the limitations.","title":"Need all possibility?"},{"location":"contribute/","text":"Contribute Get the latest version Clone the git repository $ git clone --recurse-submodules https://github.com/pprados/virtual-dataframe.git Installation Go inside the directory and $ make configure $ conda activate virtual_dataframe $ make Tests To test the project $ make test To validate the typing $ make typing To validate all the project $ make validate Project Organization \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 docs <- A default mkdocs \u251c\u2500\u2500 conda_recipe <- Script to build the conda package \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .[tests]) \u2502 so sources can be imported and dependencies installed \u251c\u2500\u2500 virtual_dataframe <- Source code for use in this project \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2514\u2500\u2500 *.py <- Framework codes \u2502 \u2514\u2500\u2500 tests <- Unit and integrations tests ((Mark directory as a sources root).","title":"Contribute"},{"location":"contribute/#contribute","text":"","title":"Contribute"},{"location":"contribute/#get-the-latest-version","text":"Clone the git repository $ git clone --recurse-submodules https://github.com/pprados/virtual-dataframe.git","title":"Get the latest version"},{"location":"contribute/#installation","text":"Go inside the directory and $ make configure $ conda activate virtual_dataframe $ make","title":"Installation"},{"location":"contribute/#tests","text":"To test the project $ make test To validate the typing $ make typing To validate all the project $ make validate","title":"Tests"},{"location":"contribute/#project-organization","text":"\u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 docs <- A default mkdocs \u251c\u2500\u2500 conda_recipe <- Script to build the conda package \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .[tests]) \u2502 so sources can be imported and dependencies installed \u251c\u2500\u2500 virtual_dataframe <- Source code for use in this project \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2514\u2500\u2500 *.py <- Framework codes \u2502 \u2514\u2500\u2500 tests <- Unit and integrations tests ((Mark directory as a sources root).","title":"Project Organization"},{"location":"faq/","text":"FAQ The code run with X, but not with Y ? You must use only the similare functionality, and only a subpart of Pandas. Develop for dask_cudf . it's easier to be compatible with others frameworks. .compute() is not defined with pandas, cudf ? If your @delayed function return something, other than a VDataFrame or VSerie , the objet has not the method .compute() . You can solve this, with: @delayed def f()-> int: return 42 real_result,=compute(f()) # Warning, compute return a tuple. The comma is important. a,b = compute(f(),f()) With CUDA, I receive NVMLError_NoPermission It's a problem with Dask. You have not the privilege to ask the size of memory for the GPU. To resolve that, add in dask configuration files : the parameter distributed:diagnostics:nvml = False the parameter worker:diagnostics:nvml = False the parameter local:device_memory_limit = 5g or the parameter --device-memory-limit 5g when you start dask-cuda-worker","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#the-code-run-with-x-but-not-with-y","text":"You must use only the similare functionality, and only a subpart of Pandas. Develop for dask_cudf . it's easier to be compatible with others frameworks.","title":"The code run with X, but not with Y ?"},{"location":"faq/#compute-is-not-defined-with-pandas-cudf","text":"If your @delayed function return something, other than a VDataFrame or VSerie , the objet has not the method .compute() . You can solve this, with: @delayed def f()-> int: return 42 real_result,=compute(f()) # Warning, compute return a tuple. The comma is important. a,b = compute(f(),f())","title":".compute() is not defined with pandas, cudf ?"},{"location":"faq/#with-cuda-i-receive-nvmlerror_nopermission","text":"It's a problem with Dask. You have not the privilege to ask the size of memory for the GPU. To resolve that, add in dask configuration files : the parameter distributed:diagnostics:nvml = False the parameter worker:diagnostics:nvml = False the parameter local:device_memory_limit = 5g or the parameter --device-memory-limit 5g when you start dask-cuda-worker","title":"With CUDA, I receive NVMLError_NoPermission"},{"location":"install/","text":"Installation Installing with Conda (recommended) $ conda install -q -y \\ -c rapidsai -c nvidia -c conda-forge \\ 'virtual_dataframe-all' or, for only one mode: $ VDF_MODE=... $ conda install -q -y \\ -c rapidsai -c nvidia -c conda-forge \\ virtual_dataframe-$VDF_MODE The package virtual_dataframe (without suffix) has no framework dependencies. It's important to understand the implication of the dependencies (see below). Installing with pip With PIP, it's not possible to install NVidia Rapids to use the GPU. A limited list of dependencies is possible. :warning: Warning: At this time, the packages are not published in pip or conda repositories Use $ pip install \"virtual_dataframe[all]@git+https://github.com/pprados/virtual-dataframe\" When the project were published, use $ pip install \"virtual_dataframe[all]\" or, for a selected framework $ VDF_MODE=... # pandas, modin, dask or dask_modin only $ pip install \"virtual_dataframe[$VDF_MODE]\" The core of the framework can be installed with $ pip install \"virtual_dataframe\" It's important to understand the implication of the dependencies (see below). Installing from the GitHub master branch $ pip install \"virtual_dataframe[all]@git+https://github.com/pprados/virtual-dataframe\" or, for a selected framework $ VDF_MODE=... # pandas, modin, dask or dask_modin only $ pip install \"virtual_dataframe[$VDF_MODE]@git+https://github.com/pprados/virtual-dataframe\" The core of the framework can be installed with $ pip install \"virtual_dataframe@git+https://github.com/pprados/virtual-dataframe\" Dependencies It's difficulte to use all the pandas like framework at the same time, in the same projet. Each framework need some dependencies, differents of the others. But, with a specific selection of versions of each framework, it's possible to merge all the frameworks. We declare this dependencies with the defaults version of virtual_dataframe . Then, you can change only the environment variable VDF_MODE to test the differents frameworks. Else, it's possible to create a conda environement for each framework, with the last version of each one. Then you must activate the specific environment and set the corresponding VDF_MODE variable. :warning: Warning: At this time, the packages are not published in pip or conda repositories #!/usr/bin/env bash # alias conda=mamba VDF_MODES=pandas modin cudf dask dask_modin dask_cudf for mode in $VDF_MODES do conda create -y -n test-$mode \\ virtual_dataframe-$mode CONDA_PREFIX=$CONDA_HOME/envs/test-$mode \\ conda env config vars set VDF_MODE=$mode done Then, you can activate the specific environment to test your code. $ conda activate test-cudf","title":"Install"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#installing-with-conda-recommended","text":"$ conda install -q -y \\ -c rapidsai -c nvidia -c conda-forge \\ 'virtual_dataframe-all' or, for only one mode: $ VDF_MODE=... $ conda install -q -y \\ -c rapidsai -c nvidia -c conda-forge \\ virtual_dataframe-$VDF_MODE The package virtual_dataframe (without suffix) has no framework dependencies. It's important to understand the implication of the dependencies (see below).","title":"Installing with Conda (recommended)"},{"location":"install/#installing-with-pip","text":"With PIP, it's not possible to install NVidia Rapids to use the GPU. A limited list of dependencies is possible. :warning: Warning: At this time, the packages are not published in pip or conda repositories Use $ pip install \"virtual_dataframe[all]@git+https://github.com/pprados/virtual-dataframe\" When the project were published, use $ pip install \"virtual_dataframe[all]\" or, for a selected framework $ VDF_MODE=... # pandas, modin, dask or dask_modin only $ pip install \"virtual_dataframe[$VDF_MODE]\" The core of the framework can be installed with $ pip install \"virtual_dataframe\" It's important to understand the implication of the dependencies (see below).","title":"Installing with pip"},{"location":"install/#installing-from-the-github-master-branch","text":"$ pip install \"virtual_dataframe[all]@git+https://github.com/pprados/virtual-dataframe\" or, for a selected framework $ VDF_MODE=... # pandas, modin, dask or dask_modin only $ pip install \"virtual_dataframe[$VDF_MODE]@git+https://github.com/pprados/virtual-dataframe\" The core of the framework can be installed with $ pip install \"virtual_dataframe@git+https://github.com/pprados/virtual-dataframe\"","title":"Installing from the GitHub master branch"},{"location":"install/#dependencies","text":"It's difficulte to use all the pandas like framework at the same time, in the same projet. Each framework need some dependencies, differents of the others. But, with a specific selection of versions of each framework, it's possible to merge all the frameworks. We declare this dependencies with the defaults version of virtual_dataframe . Then, you can change only the environment variable VDF_MODE to test the differents frameworks. Else, it's possible to create a conda environement for each framework, with the last version of each one. Then you must activate the specific environment and set the corresponding VDF_MODE variable. :warning: Warning: At this time, the packages are not published in pip or conda repositories #!/usr/bin/env bash # alias conda=mamba VDF_MODES=pandas modin cudf dask dask_modin dask_cudf for mode in $VDF_MODES do conda create -y -n test-$mode \\ virtual_dataframe-$mode CONDA_PREFIX=$CONDA_HOME/envs/test-$mode \\ conda env config vars set VDF_MODE=$mode done Then, you can activate the specific environment to test your code. $ conda activate test-cudf","title":"Dependencies"}]}