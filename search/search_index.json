{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Motivation With Panda like dataframe, do you want to create a code, and choose at the end, the framework to use? Do you want to be able to choose the best framework after simply performing performance measurements? This framework unifies multiple Panda-compatible components, to allow the writing of a single code, compatible with all. Synopsis With some parameters and Virtual classes, it's possible to write a code, and execute this code: With or without multicore With or without cluster (multi nodes on Dask or Spark) With or without GPU To do that, we create some virtual classes, add some methods in others classes, etc. To reduce the confusion, you must use the classes VDataFrame and VSeries (The prefix V is for Virtual ). These classes propose the methods .to_pandas() and .compute() for each version, but are the real classes of the selected framework. With some parameters, the real classes may be pandas.DataFrame , modin.pandas.DataFrame , cudf.DataFrame , pyspark.pandas.DataFrame without GPU, pyspark.pandas.DataFrame with GPU, dask.dataframe.DataFrame with Pandas or dask.dataframe.DataFrame with cudf (with Pandas or cudf for each partition). A new @delayed annotation can be use, with or without Dask. To manage the initialisation of a Dask ou Spark, you must use the VClient() , a connector to the cluster. This alias, can be automatically initialized with some environment variables. # Sample of code, compatible Pandas, cudf, dask, dask_modin and dask_cudf from virtual_dataframe import * TestDF = VDataFrame with (VClient()): @delayed def my_function(data: TestDF) -> TestDF: return data rc = my_function(VDataFrame({\"data\": [1, 2]}, npartitions=2)) print(rc.to_pandas()) With this framework, you can select your environment, to run or debug your code. env Environement VDF_MODE=pandas Only Python with classical pandas VDF_MODE=cudf Python with local cuDF (GPU) VDF_MODE=dask Dask with local multiple process and pandas VDF_MODE=dask_cudf Dask with local multiple process and cuDF VDF_MODE=dask DEBUG=True Dask with single thread and pandas VDF_MODE=dask_cudf DEBUG=True Dask with single thread and cuDF VDF_MODE=dask VDF_CLUSTER=dask://.local Dask with local cluster and pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://.local Dask with local cuda cluster and cuDF VDF_MODE=dask VDF_CLUSTER=dask://...:ppp Dask with remote cluster and Pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://...:ppp Dask with remote cluster and cuDF VDF_MODE=dask_modin Dask with modin VDF_MODE=dask_modin VDF_CLUSTER=dask://.local Dask with local cluster and modin VDF_MODE=dask_modin VDF_CLUSTER=dask://...:ppp Dask with remote cluster and modin VDF_MODE=pyspark VDF_CLUSTER=spark://.local PySpark with local cluster and modin VDF_MODE=pyspark VDF_CLUSTER=spark://...:ppp PySpark with remote cluster and modin For pyspark with GPU, read this . The real compatibilty between the differents simulation of Pandas, depends on the implement of the modin, cudf, pyspark or dask. Sometime, you can use the VDF_MODE variable, to update some part of code, between the selected backend. It's not always easy to write a code compatible with all scenario, but it's possible. Generally, add just .compute() and/or .to_pandas() at the end of the ETL, is enough. But, you must use, only the common feature with all frameworks. After this effort, it's possible to compare the performance about the differents technologies, or propose a component, compatible with differents scenario. For the deployment of your project, you can select the best framework for your process (in a dockerfile? or virtual environment), with only one ou two environment variables. With conda environment, you can use variables .","title":"Home"},{"location":"#motivation","text":"With Panda like dataframe, do you want to create a code, and choose at the end, the framework to use? Do you want to be able to choose the best framework after simply performing performance measurements? This framework unifies multiple Panda-compatible components, to allow the writing of a single code, compatible with all.","title":"Motivation"},{"location":"#synopsis","text":"With some parameters and Virtual classes, it's possible to write a code, and execute this code: With or without multicore With or without cluster (multi nodes on Dask or Spark) With or without GPU To do that, we create some virtual classes, add some methods in others classes, etc. To reduce the confusion, you must use the classes VDataFrame and VSeries (The prefix V is for Virtual ). These classes propose the methods .to_pandas() and .compute() for each version, but are the real classes of the selected framework. With some parameters, the real classes may be pandas.DataFrame , modin.pandas.DataFrame , cudf.DataFrame , pyspark.pandas.DataFrame without GPU, pyspark.pandas.DataFrame with GPU, dask.dataframe.DataFrame with Pandas or dask.dataframe.DataFrame with cudf (with Pandas or cudf for each partition). A new @delayed annotation can be use, with or without Dask. To manage the initialisation of a Dask ou Spark, you must use the VClient() , a connector to the cluster. This alias, can be automatically initialized with some environment variables. # Sample of code, compatible Pandas, cudf, dask, dask_modin and dask_cudf from virtual_dataframe import * TestDF = VDataFrame with (VClient()): @delayed def my_function(data: TestDF) -> TestDF: return data rc = my_function(VDataFrame({\"data\": [1, 2]}, npartitions=2)) print(rc.to_pandas()) With this framework, you can select your environment, to run or debug your code. env Environement VDF_MODE=pandas Only Python with classical pandas VDF_MODE=cudf Python with local cuDF (GPU) VDF_MODE=dask Dask with local multiple process and pandas VDF_MODE=dask_cudf Dask with local multiple process and cuDF VDF_MODE=dask DEBUG=True Dask with single thread and pandas VDF_MODE=dask_cudf DEBUG=True Dask with single thread and cuDF VDF_MODE=dask VDF_CLUSTER=dask://.local Dask with local cluster and pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://.local Dask with local cuda cluster and cuDF VDF_MODE=dask VDF_CLUSTER=dask://...:ppp Dask with remote cluster and Pandas VDF_MODE=dask_cudf VDF_CLUSTER=dask://...:ppp Dask with remote cluster and cuDF VDF_MODE=dask_modin Dask with modin VDF_MODE=dask_modin VDF_CLUSTER=dask://.local Dask with local cluster and modin VDF_MODE=dask_modin VDF_CLUSTER=dask://...:ppp Dask with remote cluster and modin VDF_MODE=pyspark VDF_CLUSTER=spark://.local PySpark with local cluster and modin VDF_MODE=pyspark VDF_CLUSTER=spark://...:ppp PySpark with remote cluster and modin For pyspark with GPU, read this . The real compatibilty between the differents simulation of Pandas, depends on the implement of the modin, cudf, pyspark or dask. Sometime, you can use the VDF_MODE variable, to update some part of code, between the selected backend. It's not always easy to write a code compatible with all scenario, but it's possible. Generally, add just .compute() and/or .to_pandas() at the end of the ETL, is enough. But, you must use, only the common feature with all frameworks. After this effort, it's possible to compare the performance about the differents technologies, or propose a component, compatible with differents scenario. For the deployment of your project, you can select the best framework for your process (in a dockerfile? or virtual environment), with only one ou two environment variables. With conda environment, you can use variables .","title":"Synopsis"},{"location":"CHANGELOG/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [v0.5] - 2022-11-15 First version published on the repositories (pip and conda-forge). Added Pandas, cudf, modin, dask, dask_cudf, dask_modin, pyspark and pyspark+rapids frameworks Changed Nop Deprecated Nop Removed Nop Fixed Nop Security Nop","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#v05-2022-11-15","text":"First version published on the repositories (pip and conda-forge).","title":"[v0.5] - 2022-11-15"},{"location":"CHANGELOG/#added","text":"Pandas, cudf, modin, dask, dask_cudf, dask_modin, pyspark and pyspark+rapids frameworks","title":"Added"},{"location":"CHANGELOG/#changed","text":"Nop","title":"Changed"},{"location":"CHANGELOG/#deprecated","text":"Nop","title":"Deprecated"},{"location":"CHANGELOG/#removed","text":"Nop","title":"Removed"},{"location":"CHANGELOG/#fixed","text":"Nop","title":"Fixed"},{"location":"CHANGELOG/#security","text":"Nop","title":"Security"},{"location":"api/","text":"API api comments vdf.@delayed Delayed function (do nothing or dask.delayed) vdf.concat(...) Merge VDataFrame vdf.read_csv(...) Read VDataFrame from CSVs glob files vdf.read_excel(...) * Read VDataFrame from Excel glob files vdf.read_fwf(...) * Read VDataFrame from Fwf glob files vdf.read_hdf(...) * Read VDataFrame from HDFs glob files vdf.read_json(...) Read VDataFrame from Jsons glob files vdf.read_orc(...) Read VDataFrame from ORCs glob files vdf.read_parquet(...) Read VDataFrame from Parquets glob files vdf.read_sql_table(...) * Read VDataFrame from SQL vdf.from_pandas(pdf, npartitions=...) Create Virtual Dataframe from Pandas DataFrame vdf.from_backend(vdf, npartitions=...) Create Virtual Dataframe from backend dataframe vdf.compute([...]) Compute multiple @delayed functions VDataFrame(data, npartitions=...) Create DataFrame in memory (only for test) VSeries(data, npartitions=...) Create Series in memory (only for test) VLocalCluster(...) Create a dask Local Cluster (Dask, Cuda or Spark) BackEndDataFrame The class of dask backend dataframe BackEndSeries The class of dask backend series BackEnd The backend framework VDataFrame.compute() Compute the virtual dataframe VDataFrame.persist() Persist the dataframe in memory VDataFrame.repartition() Rebalance the dataframe VDataFrame.visualize() Create an image with the graph VDataFrame.to_pandas() Convert to pandas dataframe VDataFrame.to_csv() Save to glob files VDataFrame.to_excel() * Save to glob files VDataFrame.to_feather() * Save to glob files VDataFrame.to_hdf() * Save to glob files VDataFrame.to_json() Save to glob files VDataFrame.to_orc() Save to glob files VDataFrame.to_parquet() Save to glob files VDataFrame.to_sql() * Save to sql table VDataFrame.to_numpy() Convert to numpy array VDataFrame.to_backend() Convert to backend dataframe VDataFrame.categorize() Detect all categories VDataFrame.apply_rows() Apply rows, GPU template VDataFrame.map_partitions() Apply function for each parttions VSeries.compute() Compute the virtual series VSeries.persist() Persist the dataframe in memory VSeries.repartition() Rebalance the dataframe VSeries.visualize() Create an image with the graph VSeries.to_pandas() Convert to pandas series VSeries.to_backend() Convert to backend series VSeries.to_numpy() Convert to numpy array VClient(...) The connexion with the cluster VDF_MODE The current mode Mode The enumeration of differents mode * some frameworks do not implement it You can read a sample notebook here for an exemple of all API. Each API propose a specific version for each framework. For example: the toPandas() with Panda, return self @delayed use the dask @delayed or do nothing, and apply the code when the function was called. In the first case, the function return a part of the dask graph. In the second case, the function return immediately the result. read_csv(\"file*.csv\") read each file in parallele with dask or spark by each worker, or read sequencially each file and combine each dataframe with a framework without distribution (pandas, modin, cudf) save_csv(\"file*.csv\") write each file in parallele with dask or spark, or write one file \"file.csv\" (without the star) with a framework without distribution (pandas, modin, cudf) ...","title":"API"},{"location":"api/#api","text":"api comments vdf.@delayed Delayed function (do nothing or dask.delayed) vdf.concat(...) Merge VDataFrame vdf.read_csv(...) Read VDataFrame from CSVs glob files vdf.read_excel(...) * Read VDataFrame from Excel glob files vdf.read_fwf(...) * Read VDataFrame from Fwf glob files vdf.read_hdf(...) * Read VDataFrame from HDFs glob files vdf.read_json(...) Read VDataFrame from Jsons glob files vdf.read_orc(...) Read VDataFrame from ORCs glob files vdf.read_parquet(...) Read VDataFrame from Parquets glob files vdf.read_sql_table(...) * Read VDataFrame from SQL vdf.from_pandas(pdf, npartitions=...) Create Virtual Dataframe from Pandas DataFrame vdf.from_backend(vdf, npartitions=...) Create Virtual Dataframe from backend dataframe vdf.compute([...]) Compute multiple @delayed functions VDataFrame(data, npartitions=...) Create DataFrame in memory (only for test) VSeries(data, npartitions=...) Create Series in memory (only for test) VLocalCluster(...) Create a dask Local Cluster (Dask, Cuda or Spark) BackEndDataFrame The class of dask backend dataframe BackEndSeries The class of dask backend series BackEnd The backend framework VDataFrame.compute() Compute the virtual dataframe VDataFrame.persist() Persist the dataframe in memory VDataFrame.repartition() Rebalance the dataframe VDataFrame.visualize() Create an image with the graph VDataFrame.to_pandas() Convert to pandas dataframe VDataFrame.to_csv() Save to glob files VDataFrame.to_excel() * Save to glob files VDataFrame.to_feather() * Save to glob files VDataFrame.to_hdf() * Save to glob files VDataFrame.to_json() Save to glob files VDataFrame.to_orc() Save to glob files VDataFrame.to_parquet() Save to glob files VDataFrame.to_sql() * Save to sql table VDataFrame.to_numpy() Convert to numpy array VDataFrame.to_backend() Convert to backend dataframe VDataFrame.categorize() Detect all categories VDataFrame.apply_rows() Apply rows, GPU template VDataFrame.map_partitions() Apply function for each parttions VSeries.compute() Compute the virtual series VSeries.persist() Persist the dataframe in memory VSeries.repartition() Rebalance the dataframe VSeries.visualize() Create an image with the graph VSeries.to_pandas() Convert to pandas series VSeries.to_backend() Convert to backend series VSeries.to_numpy() Convert to numpy array VClient(...) The connexion with the cluster VDF_MODE The current mode Mode The enumeration of differents mode * some frameworks do not implement it You can read a sample notebook here for an exemple of all API. Each API propose a specific version for each framework. For example: the toPandas() with Panda, return self @delayed use the dask @delayed or do nothing, and apply the code when the function was called. In the first case, the function return a part of the dask graph. In the second case, the function return immediately the result. read_csv(\"file*.csv\") read each file in parallele with dask or spark by each worker, or read sequencially each file and combine each dataframe with a framework without distribution (pandas, modin, cudf) save_csv(\"file*.csv\") write each file in parallele with dask or spark, or write one file \"file.csv\" (without the star) with a framework without distribution (pandas, modin, cudf) ...","title":"API"},{"location":"bench/","text":"Bench To make a bench between framework, you must identify three step: - Time to start the local cluster , if any. - Time to compile the first time, the python code to C or GPU - Time to run the performance tests Our recommandation it to run one time your test, and only after, run multiple time the same performance tests and calculate the timeit magic command. The Python 3.11 need the same approach.","title":"Bench"},{"location":"bench/#bench","text":"To make a bench between framework, you must identify three step: - Time to start the local cluster , if any. - Time to compile the first time, the python code to C or GPU - Time to run the performance tests Our recommandation it to run one time your test, and only after, run multiple time the same performance tests and calculate the timeit magic command. The Python 3.11 need the same approach.","title":"Bench"},{"location":"best_practices/","text":"Best practices For write a code, optimized with all frameworks, you must use some best practices . Avoid very large partition: so that they fit in a worker's available memory. Avoid very large graphs: because that can create an overhead on task. Learn techniques for customization: in order to improve the efficiency of your processes. Stop using Dask when no longer needed: like when you are iterating on a much smaller amount of data. Persist in dsitributed RAM when you can: in doing so, accessing RAM memory will be faster. Processes and threads: be careful to separate numeric work from text data to maintain efficiency. Load data with Dask or Spark : for instance, if you need to work with large Python objects, let Dask create them (instead of creating them outside Dask or Spark and then handing thom over the framework). Avoid using VDataFrame or VSeries outside the unit tests Avoid calling compute repeatedly: as this can lower performance. Avoid iterate over rows or columns of DataFrame: Use apply() or apply_rows() to distribute the code in the cluster and GPU. If you know that the volume of data is compatible with one node, you can convert a distributed dataframe to BackendDataFrame and continue to manipulate the data locally. Use the .to_backend() to convert a Dataframe to Pandas or cudf Dataframe.","title":"Best Practice"},{"location":"best_practices/#best-practices","text":"For write a code, optimized with all frameworks, you must use some best practices . Avoid very large partition: so that they fit in a worker's available memory. Avoid very large graphs: because that can create an overhead on task. Learn techniques for customization: in order to improve the efficiency of your processes. Stop using Dask when no longer needed: like when you are iterating on a much smaller amount of data. Persist in dsitributed RAM when you can: in doing so, accessing RAM memory will be faster. Processes and threads: be careful to separate numeric work from text data to maintain efficiency. Load data with Dask or Spark : for instance, if you need to work with large Python objects, let Dask create them (instead of creating them outside Dask or Spark and then handing thom over the framework). Avoid using VDataFrame or VSeries outside the unit tests Avoid calling compute repeatedly: as this can lower performance. Avoid iterate over rows or columns of DataFrame: Use apply() or apply_rows() to distribute the code in the cluster and GPU. If you know that the volume of data is compatible with one node, you can convert a distributed dataframe to BackendDataFrame and continue to manipulate the data locally. Use the .to_backend() to convert a Dataframe to Pandas or cudf Dataframe.","title":"Best practices"},{"location":"cluster/","text":"Cluster To connect to a cluster, use VDF_CLUSTER with protocol, host and optionaly, the port. dask://locahost:8787 spark://locahost:7077 or alternativelly, use DASK_SCHEDULER_SERVICE_HOST and DASK_SCHEDULER_SERVICE_PORT or SPARK_MASTER_HOST and SPARK_MASTER_PORT VDF_MODE DEBUG VDF_CLUSTER Scheduler pandas - - No scheduler cudf - - No scheduler modin - - No scheduler dask Yes - synchronous dask No - thread dask No dask://threads thread dask No dask://processes processes dask No dask://.local LocalCluster dask_modin No - LocalCluster dask_modin No dask://.local LocalCluster dask_modin No dask://<host>:<port> Dask cluster dask_cudf No dask://.local LocalCUDACluster dask_cudf No dask://<host>:<port> Dask cluster pyspark No spark:local[*] Spark local cluster pyspark No spark://.local Spark local cluster pyspark No spark://<host>:<port> Spark cluster The special host name , ends with .local can be used to start a LocalCluster , LocalCUDACluster or Spark local[*] when your program is started. An instance of local cluster is started and injected in the Client . Sample: from virtual_dataframe import VClient with VClient(): # Now, use the scheduler pass If you want to manage the parameters of Local(CUDA)Cluster or SparkCluster , use the alternative VLocalCluster() . from virtual_dataframe import VClient,VLocalCluster with VClient(VLocalCluster(params=...)): # Now, use the scheduler pass Dask local cluster To update the parameters for the implicit Local(CUDA)Cluster , you can use the Dask config file . local: scheduler-port: 0 device_memory_limit: 5G you can set some environment variables for dask, export DASK_LOCAL__SCHEDULER_PORT=0 export DASK_LOCAL__DEVICE_MEMORY_LIMIT=5g or for Domino datalab, export DASK_SCHEDULER_SERVICE_HOST=... export DASK_SCHEDULER_SERVICE_PORT=7077 Spark cluster To configure the spark cluster, - use a file spark.conf with the Spark properties use environment variables like export spark.app.name=MyApp . for VLocalCluster , use classical parameters, and replace dot to _ : from virtual_dataframe import VClient,VLocalCluster with VClient(VLocalCluster( spark_app_name=\"MyApp\", spark_master=\"local[*]\", )): # Now, use the scheduler pass or for Domino datalab, export SPARK_MASTER_HOST=... export SPARK_MASTER_PORT=7077 Spark cluster with GPU To use the Spark+rapids , download the file rapids-4-spark_2.12-22.10.0.jar (see here ). Then, in the file spark.conf , add: spark.jars=rapids-4-spark_2.12-22.10.0.jar spark.plugins=com.nvidia.spark.SQLPlugin spark.rapids.sql.concurrentGpuTasks=1","title":"Cluster"},{"location":"cluster/#cluster","text":"To connect to a cluster, use VDF_CLUSTER with protocol, host and optionaly, the port. dask://locahost:8787 spark://locahost:7077 or alternativelly, use DASK_SCHEDULER_SERVICE_HOST and DASK_SCHEDULER_SERVICE_PORT or SPARK_MASTER_HOST and SPARK_MASTER_PORT VDF_MODE DEBUG VDF_CLUSTER Scheduler pandas - - No scheduler cudf - - No scheduler modin - - No scheduler dask Yes - synchronous dask No - thread dask No dask://threads thread dask No dask://processes processes dask No dask://.local LocalCluster dask_modin No - LocalCluster dask_modin No dask://.local LocalCluster dask_modin No dask://<host>:<port> Dask cluster dask_cudf No dask://.local LocalCUDACluster dask_cudf No dask://<host>:<port> Dask cluster pyspark No spark:local[*] Spark local cluster pyspark No spark://.local Spark local cluster pyspark No spark://<host>:<port> Spark cluster The special host name , ends with .local can be used to start a LocalCluster , LocalCUDACluster or Spark local[*] when your program is started. An instance of local cluster is started and injected in the Client . Sample: from virtual_dataframe import VClient with VClient(): # Now, use the scheduler pass If you want to manage the parameters of Local(CUDA)Cluster or SparkCluster , use the alternative VLocalCluster() . from virtual_dataframe import VClient,VLocalCluster with VClient(VLocalCluster(params=...)): # Now, use the scheduler pass","title":"Cluster"},{"location":"cluster/#dask-local-cluster","text":"To update the parameters for the implicit Local(CUDA)Cluster , you can use the Dask config file . local: scheduler-port: 0 device_memory_limit: 5G you can set some environment variables for dask, export DASK_LOCAL__SCHEDULER_PORT=0 export DASK_LOCAL__DEVICE_MEMORY_LIMIT=5g or for Domino datalab, export DASK_SCHEDULER_SERVICE_HOST=... export DASK_SCHEDULER_SERVICE_PORT=7077","title":"Dask local cluster"},{"location":"cluster/#spark-cluster","text":"To configure the spark cluster, - use a file spark.conf with the Spark properties use environment variables like export spark.app.name=MyApp . for VLocalCluster , use classical parameters, and replace dot to _ : from virtual_dataframe import VClient,VLocalCluster with VClient(VLocalCluster( spark_app_name=\"MyApp\", spark_master=\"local[*]\", )): # Now, use the scheduler pass or for Domino datalab, export SPARK_MASTER_HOST=... export SPARK_MASTER_PORT=7077","title":"Spark cluster"},{"location":"cluster/#spark-cluster-with-gpu","text":"To use the Spark+rapids , download the file rapids-4-spark_2.12-22.10.0.jar (see here ). Then, in the file spark.conf , add: spark.jars=rapids-4-spark_2.12-22.10.0.jar spark.plugins=com.nvidia.spark.SQLPlugin spark.rapids.sql.concurrentGpuTasks=1","title":"Spark cluster with GPU"},{"location":"commands/","text":"Make commands The Makefile contains the central entry points for common tasks related to this project. Commands make help will print all majors target make configure will prepare the environment (conda venv, kernel, ...) make lint will lint the code make test will run all tests make typing will check the typing make validate will validate the version before commit make clean will clean current environment make docs will create and show a HTML documentation in 'build/' make dist will create a full wheel distribution Jupyter commands make notebook will start a jupyter notebook make remove-kernel will remove the project's kernel make nb-run-% will execute all notebooks make nb-convert will convert all notebooks to python make clean-notebooks will clean all datas in the notebooks Twine commands make check-twine will check the packaging before publication make test-twine will publish the package in test.pypi.org <https://test.pypi.org> _) make twine will publish the package in pypi.org <https://pypi.org> _) Conda commands make conda-build will build the conda package make conda-debug will build the package in debug mode make conda-convert will convert the package for all platform make conda-install will install the conda package make conda-purge will remove the build artefact make conda-create will create a test environment for each mode make test-conda-forge will test the conda-forge first step publication make conda-forge will publish the project to conda-forge","title":"Commands"},{"location":"commands/#make-commands","text":"The Makefile contains the central entry points for common tasks related to this project.","title":"Make commands"},{"location":"commands/#commands","text":"make help will print all majors target make configure will prepare the environment (conda venv, kernel, ...) make lint will lint the code make test will run all tests make typing will check the typing make validate will validate the version before commit make clean will clean current environment make docs will create and show a HTML documentation in 'build/' make dist will create a full wheel distribution","title":"Commands"},{"location":"commands/#jupyter-commands","text":"make notebook will start a jupyter notebook make remove-kernel will remove the project's kernel make nb-run-% will execute all notebooks make nb-convert will convert all notebooks to python make clean-notebooks will clean all datas in the notebooks","title":"Jupyter commands"},{"location":"commands/#twine-commands","text":"make check-twine will check the packaging before publication make test-twine will publish the package in test.pypi.org <https://test.pypi.org> _) make twine will publish the package in pypi.org <https://pypi.org> _)","title":"Twine commands"},{"location":"commands/#conda-commands","text":"make conda-build will build the conda package make conda-debug will build the package in debug mode make conda-convert will convert the package for all platform make conda-install will install the conda package make conda-purge will remove the build artefact make conda-create will create a test environment for each mode make test-conda-forge will test the conda-forge first step publication make conda-forge will publish the project to conda-forge","title":"Conda commands"},{"location":"compatibility/","text":"Compatibility This project is just a wrapper. So, it inherits limitations and bugs from other projects. Sorry for that. Limitations pandas All data must be in DRAM modin Read this cudf All data must be in VRAM All data types in cuDF are nullable Iterating over a cuDF Series, DataFrame or Index is not supported. Join (or merge) and groupby operations in cuDF do not guarantee output ordering. The order of operations is not always deterministic Cudf does not support duplicate column names Cudf also supports .apply() it relies on Numba to JIT compile the UDF and execute it on the GPU .apply(result_type=...) not supported dask transpose() and MultiIndex are not implemented Column assignment doesn't support type list dask_cudf See cudf and dask. Categories with strings not implemented pyspark Read this File format compatibility To be compatible with all framework, you must only use the common features. We accept some function to read or write files, but we write a warning if you use a function not compatible with others frameworks. read_... / to_... pandas cudf modin dask dask_modin dask_cudf pyspark vdf.read_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_excel \u2713 \u2713 \u2713 VDataFrame.to_excel \u2713 \u2713 \u2713 VSeries.to_excel \u2713 \u2713 \u2713 vdf.read_feather \u2713 \u2713 \u2713 VDataFrame.to_feather \u2713 \u2713 \u2713 vdf.read_fwf \u2713 \u2713 \u2713 \u2713 vdf.read_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_sql_table \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_sql \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_sql \u2713 \u2713 \u2713 \u2713 \u2713 Cross framework compatibility small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask, dask_modin or pyspark Limits:++ GPU cudf Limits:++ dask_cudf, pyspark+spark-rapids Limits:+++ To develop, you can choose the level to be compatible with others frameworks. Each cell is strongly compatible with the upper left part. No need of GPU? If you don't need to use a GPU, then develop for dask and use mode in bold . small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask, dask_modin or pyspark Limits:++ GPU cudf Limits:++ dask_cudf, pyspark+spark-rapids Limits:+++ You can ignore this API: VDataFrame.apply_rows() No need of big data? If you don't need to use big data, then develop for cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask, dask_modin or pyspark Limits:++ GPU cudf Limits:++ dask_cudf, pyspark+spark-rapids Limits:+++ You can ignore these API: @delayed map_partitions() categorize() compute() npartitions=... Need all possibility? To be compatible with all modes, develop for dask_cudf . small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask, dask_modin or pyspark Limits:++ GPU cudf Limits:++ dask_cudf , pyspark+spark-rapids Limits:+++ and accept all the limitations.","title":"Compatibility"},{"location":"compatibility/#compatibility","text":"This project is just a wrapper. So, it inherits limitations and bugs from other projects. Sorry for that. Limitations pandas All data must be in DRAM modin Read this cudf All data must be in VRAM All data types in cuDF are nullable Iterating over a cuDF Series, DataFrame or Index is not supported. Join (or merge) and groupby operations in cuDF do not guarantee output ordering. The order of operations is not always deterministic Cudf does not support duplicate column names Cudf also supports .apply() it relies on Numba to JIT compile the UDF and execute it on the GPU .apply(result_type=...) not supported dask transpose() and MultiIndex are not implemented Column assignment doesn't support type list dask_cudf See cudf and dask. Categories with strings not implemented pyspark Read this","title":"Compatibility"},{"location":"compatibility/#file-format-compatibility","text":"To be compatible with all framework, you must only use the common features. We accept some function to read or write files, but we write a warning if you use a function not compatible with others frameworks. read_... / to_... pandas cudf modin dask dask_modin dask_cudf pyspark vdf.read_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_csv \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_excel \u2713 \u2713 \u2713 VDataFrame.to_excel \u2713 \u2713 \u2713 VSeries.to_excel \u2713 \u2713 \u2713 vdf.read_feather \u2713 \u2713 \u2713 VDataFrame.to_feather \u2713 \u2713 \u2713 vdf.read_fwf \u2713 \u2713 \u2713 \u2713 vdf.read_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_hdf \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_json \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_orc \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_parquet \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 vdf.read_sql_table \u2713 \u2713 \u2713 \u2713 \u2713 VDataFrame.to_sql \u2713 \u2713 \u2713 \u2713 \u2713 VSeries.to_sql \u2713 \u2713 \u2713 \u2713 \u2713","title":"File format compatibility"},{"location":"compatibility/#cross-framework-compatibility","text":"small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask, dask_modin or pyspark Limits:++ GPU cudf Limits:++ dask_cudf, pyspark+spark-rapids Limits:+++ To develop, you can choose the level to be compatible with others frameworks. Each cell is strongly compatible with the upper left part.","title":"Cross framework compatibility"},{"location":"compatibility/#no-need-of-gpu","text":"If you don't need to use a GPU, then develop for dask and use mode in bold . small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask, dask_modin or pyspark Limits:++ GPU cudf Limits:++ dask_cudf, pyspark+spark-rapids Limits:+++ You can ignore this API: VDataFrame.apply_rows()","title":"No need of GPU?"},{"location":"compatibility/#no-need-of-big-data","text":"If you don't need to use big data, then develop for cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask, dask_modin or pyspark Limits:++ GPU cudf Limits:++ dask_cudf, pyspark+spark-rapids Limits:+++ You can ignore these API: @delayed map_partitions() categorize() compute() npartitions=...","title":"No need of big data?"},{"location":"compatibility/#need-all-possibility","text":"To be compatible with all modes, develop for dask_cudf . small data middle data big data 1-CPU pandas Limits:+ n-CPU modin Limits+ dask, dask_modin or pyspark Limits:++ GPU cudf Limits:++ dask_cudf , pyspark+spark-rapids Limits:+++ and accept all the limitations.","title":"Need all possibility?"},{"location":"contribute/","text":"Contribute Get the latest version Clone the git repository $ git clone --recurse-submodules https://github.com/pprados/virtual-dataframe.git Installation Go inside the directory and $ make configure $ conda activate virtual_dataframe $ make Tests To test the project $ make test To validate the typing $ make typing To validate all the project $ make validate Project Organization \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 docs <- A default mkdocs \u251c\u2500\u2500 conda_recipe <- Script to build the conda package \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .[tests]) \u2502 so sources can be imported and dependencies installed \u251c\u2500\u2500 virtual_dataframe <- Source code for use in this project \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2514\u2500\u2500 *.py <- Framework codes \u2502 \u2514\u2500\u2500 tests <- Unit and integrations tests ((Mark directory as a sources root).","title":"Contribute"},{"location":"contribute/#contribute","text":"","title":"Contribute"},{"location":"contribute/#get-the-latest-version","text":"Clone the git repository $ git clone --recurse-submodules https://github.com/pprados/virtual-dataframe.git","title":"Get the latest version"},{"location":"contribute/#installation","text":"Go inside the directory and $ make configure $ conda activate virtual_dataframe $ make","title":"Installation"},{"location":"contribute/#tests","text":"To test the project $ make test To validate the typing $ make typing To validate all the project $ make validate","title":"Tests"},{"location":"contribute/#project-organization","text":"\u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 docs <- A default mkdocs \u251c\u2500\u2500 conda_recipe <- Script to build the conda package \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .[tests]) \u2502 so sources can be imported and dependencies installed \u251c\u2500\u2500 virtual_dataframe <- Source code for use in this project \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2514\u2500\u2500 *.py <- Framework codes \u2502 \u2514\u2500\u2500 tests <- Unit and integrations tests ((Mark directory as a sources root).","title":"Project Organization"},{"location":"faq/","text":"FAQ The code run with X, but not with Y? You must use only the similar functionality, and only a subpart of Pandas. Develop for dask_cudf . it's easier to be compatible with others frameworks. .compute() is not defined with pandas, cudf? If your @delayed function return something, other than a VDataFrame or VSerie , the objet does not have the method .compute() . You can solve this, with: @delayed def f()-> int: return 42 real_result,=compute(f()) # Warning, compute return a tuple. The comma is important. a,b = compute(f(),f()) With CUDA, I receive NVMLError_NoPermission It's a problem with Dask. You have not the privilege to ask the size of memory for the GPU. To resolve that, add in dask configuration files : the parameter distributed:diagnostics:nvml = False the parameter local:device_memory_limit = 5g or the parameter --device-memory-limit 5g when you start dask-cuda-worker (update the size)","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#the-code-run-with-x-but-not-with-y","text":"You must use only the similar functionality, and only a subpart of Pandas. Develop for dask_cudf . it's easier to be compatible with others frameworks.","title":"The code run with X, but not with Y?"},{"location":"faq/#compute-is-not-defined-with-pandas-cudf","text":"If your @delayed function return something, other than a VDataFrame or VSerie , the objet does not have the method .compute() . You can solve this, with: @delayed def f()-> int: return 42 real_result,=compute(f()) # Warning, compute return a tuple. The comma is important. a,b = compute(f(),f())","title":".compute() is not defined with pandas, cudf?"},{"location":"faq/#with-cuda-i-receive-nvmlerror_nopermission","text":"It's a problem with Dask. You have not the privilege to ask the size of memory for the GPU. To resolve that, add in dask configuration files : the parameter distributed:diagnostics:nvml = False the parameter local:device_memory_limit = 5g or the parameter --device-memory-limit 5g when you start dask-cuda-worker (update the size)","title":"With CUDA, I receive NVMLError_NoPermission"},{"location":"install/","text":"Installation Installing with Conda (recommended) $ conda install -c conda-forge \"virtual_dataframe\" Installing with pip Use $ pip install \"virtual_dataframe\" Installing from the GitHub master branch $ pip install \"virtual_dataframe@git+https://github.com/pprados/virtual-dataframe\" Dependencies You must install all others frameworks to use it with virtual_dataframe . You can create a set of virtual environment, with the tools: $ build-conda-vdf-env --help like $ build-conda-vdf-env pandas cudf dask_cudf pyspark pyspark_gpu-local $ conda env list $ conda activate vdf-cudf $ conda activate vdf-dask_cudf-local The VDF_MODE is set for each environment. If you create an environment for a dask or spark framework, two environment will be created. One vdf-XXX where you must set the VDF_CLUSTER variable and another vdf-XXX-local with a pre set of VDF_CLUSTER=dask://.local or VDF_CLUSTER=spark://.local to use a local cluster. For pyspark_gpu \u0300, somes environment variables will be set, to reference the rapids-4-spark_2.12-22.10.0.jar file. You have this file in the root of your project. You can find all environement Yaml file here . You can remove all or specific versions with: $ build-conda-vdf-envs --remove","title":"Install"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#installing-with-conda-recommended","text":"$ conda install -c conda-forge \"virtual_dataframe\"","title":"Installing with Conda (recommended)"},{"location":"install/#installing-with-pip","text":"Use $ pip install \"virtual_dataframe\"","title":"Installing with pip"},{"location":"install/#installing-from-the-github-master-branch","text":"$ pip install \"virtual_dataframe@git+https://github.com/pprados/virtual-dataframe\"","title":"Installing from the GitHub master branch"},{"location":"install/#dependencies","text":"You must install all others frameworks to use it with virtual_dataframe . You can create a set of virtual environment, with the tools: $ build-conda-vdf-env --help like $ build-conda-vdf-env pandas cudf dask_cudf pyspark pyspark_gpu-local $ conda env list $ conda activate vdf-cudf $ conda activate vdf-dask_cudf-local The VDF_MODE is set for each environment. If you create an environment for a dask or spark framework, two environment will be created. One vdf-XXX where you must set the VDF_CLUSTER variable and another vdf-XXX-local with a pre set of VDF_CLUSTER=dask://.local or VDF_CLUSTER=spark://.local to use a local cluster. For pyspark_gpu \u0300, somes environment variables will be set, to reference the rapids-4-spark_2.12-22.10.0.jar file. You have this file in the root of your project. You can find all environement Yaml file here . You can remove all or specific versions with: $ build-conda-vdf-envs --remove","title":"Dependencies"},{"location":"test/","text":"Test To test your code with all scenario, you can write a Makefile : # Makefile export VDF_MODES=pandas cudf dask dask_modin dask_cudf pyspark export VDF_MODE # Run unit test with a specific *mode* .PHONY: unit-test-* .make-_unit-test-%: $(REQUIREMENTS) $(PYTHON_TST) $(PYTHON_SRC) @$(VALIDATE_VENV) echo \"Run unit tests...\" python -m pytest --rootdir=. -s tests date >.make-_unit-test-$* unit-test-%: @echo \"Test with VDF_MODE=$*\" VDF_MODE=$* $(MAKE) --no-print-directory .make-_unit-test-$* unit-test: $(foreach ext,$(VDF_MODES),unit-test-$(ext)) then make unit-test-pandas # Test only pandas make unit-test-pyspark # Test only pyspark make unit-test # Test all scenario","title":"Test"},{"location":"test/#test","text":"To test your code with all scenario, you can write a Makefile : # Makefile export VDF_MODES=pandas cudf dask dask_modin dask_cudf pyspark export VDF_MODE # Run unit test with a specific *mode* .PHONY: unit-test-* .make-_unit-test-%: $(REQUIREMENTS) $(PYTHON_TST) $(PYTHON_SRC) @$(VALIDATE_VENV) echo \"Run unit tests...\" python -m pytest --rootdir=. -s tests date >.make-_unit-test-$* unit-test-%: @echo \"Test with VDF_MODE=$*\" VDF_MODE=$* $(MAKE) --no-print-directory .make-_unit-test-$* unit-test: $(foreach ext,$(VDF_MODES),unit-test-$(ext)) then make unit-test-pandas # Test only pandas make unit-test-pyspark # Test only pyspark make unit-test # Test all scenario","title":"Test"}]}