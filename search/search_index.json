{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Virtual DataFrame Full documentation Motivation With Panda like dataframe, do you want to create a code, and choose at the end, the framework to use? Do you want to be able to choose the best framework after simply performing performance measurements? This framework unifies multiple Panda-compatible components, to allow the writing of a single code, compatible with all. Synopsis With some parameters and Virtual classes, it's possible to write a code, and execute this code: - With or without multicore - With or without cluster (multi nodes) - With or without GPU To do that, we create some virtual classes, add some methods in others classes, etc. It's difficult to use a combinaison of framework, with the same classe name, with similare semantic, etc. For example, if you want to use in the same program, Dask, cudf, pandas and modin, you must manage: - pandas.DataFrame , pandas,Series - modin.pandas.DataFrame , modin.pandas.Series - cudf.DataFrame , cudf.Series - dask.DataFrame , dask.Series With cudf , the code must call .to_pandas() . With dask, the code must call .compute() , can use @delayed or dask.distributed.Client . etc. We propose to replace all these classes and scenarios, with a uniform model , inspired by dask (the more complex API). Then, it is possible to write one code, and use it in differents environnements and frameworks. This project is essentially a back-port of Dask+Cudf to others frameworks. To reduce the confusion, you must use the classes VDataFrame and VSeries (The prefix V is for Virtual ). These classes propose the methods .to_pandas() and .compute() for each version, but are the real classes of the selected framework. A new @delayed annotation can be use, with or without Dask. With some parameters, the real classes may be pandas.DataFrame , modin.pandas.DataFrame , cudf.DataFrame , dask.dataframe.DataFrame with Pandas or dask.dataframe.DataFrame with cudf (with Pandas or cudf for each partition). To manage the initialisation of a Dask or Ray cluster, you must use the VClient() . This alias, can be automatically initialized with some environment variables. # Sample of code, compatible Pandas, cudf, modin, dask and dask_cudf from virtual_dataframe import * TestDF = VDataFrame with (VClient()): @delayed def my_function(data: TestDF) -> TestDF: return data rc = my_function(VDataFrame({\"data\": [1, 2]}, npartitions=2)) print(rc.to_pandas()) With this framework, you can select your environment, to run or debug your code. env Environement VDF_MODE=pandas Only Python with classical pandas VDF_MODE=modin Python with local modin VDF_MODE=cudf Python with local cuDF (GPU) VDF_MODE=dask Dask with local multiple process and pandas VDF_MODE=dask-cudf Dask with local multiple process and cuDF VDF_MODE=dask DEBUG=True Dask with single thread and pandas VDF_MODE=dask-cudf DEBUG=True Dask with single thread and cuDF VDF_MODE=dask VDF_CLUSTER=dask://localhost Dask with local cluster and pandas VDF_MODE=dask-cudf VDF_CLUSTER=dask://localhost Dask with local cuda cluster and cuDF VDF_MODE=dask VDF_CLUSTER=dask://...:ppp Dask with remote cluster and Pandas VDF_MODE=dask-cudf VDF_CLUSTER=dask://...:ppp Dask with remote cluster and cuDF VDF_MODE=dask-modin VDF_CLUSTER=dask://localhost Dask with local cluster and modin VDF_MODE=ray-modin VDF_CLUSTER=ray://localhost Ray with local cluster and modin VDF_MODE=dask-modin VDF_CLUSTER=dask://...:ppp Dask with remote cluster and modin VDF_MODE=ray-modin VDF_CLUSTER=ray://...:ppp Ray with remote cluster and modin The real compatibilty between the differents simulation of Pandas, depends on the implement of the modin, cudf or dask. Sometime, you can use the VDF_MODE variable, to update some part of code, between the selected backend. It's not always easy to write a code compatible with all scenario, but it's possible. Generally, add just .compute() and/or .to_pandas() at the end of the ETL, is enough. But, you must use, only the common feature with all frameworks. After this effort, it's possible to compare the performance about the differents technologies, or propose a component, compatible with differents scenario. For the deployment of your project, you can select the best framework for your process (in a dockerfile?), with only one ou two environment variable. Cluster To connect to a cluster, use VDF_CLUSTER with protocol, host and optionaly, the port. - dask://locahost:8787 - ray://locahost:10001 - ray:auto - or alternativelly, use DASK_SCHEDULER_SERVICE_HOST and DASK_SCHEDULER_SERVICE_PORT VDF_MODE DEBUG VDF_CLUSTER Scheduler pandas - - No scheduler cudf - - No scheduler modin - - No scheduler dask Yes - synchronous dask No - processes dask No dask://localhost LocalCluster dask_modin No - LocalCluster dask_modin No dask://localhost LocalCluster dask_modin No dask:// Dask cluster ray_modin No ray:auto Dask cluster ray_modin No ray://localhost Dask cluster ray_modin No ray:// Dask cluster dask-cudf No dask://localhost LocalCUDACluster dask-cudf No dask:// Dask cluster Sample: from virtual_dataframe import VClient with (VClient()) # Now, use the scheduler Installation Warning: At this time, the packages are not published in pip or conda repositories Installing with Conda (recommended) $ conda install -q -y \\ -c rapidsai -c nvidia -c conda-forge \\ \"virtual_dataframe-all\" or, for only one mode: $ VDF_MODE=... $ conda install -q -y \\ -c rapidsai -c nvidia -c conda-forge \\ virtual_dataframe-$VDF_MODE The package virtual_dataframe (without suffix) has no framework dependencies. Installing with pip With PIP, it's not possible to install NVidia Rapids to use the GPU. A limited list of dependencies is possible. $ pip install \"virtual_dataframe[all]\" or, for a selected framework $ VDF_MODE=... # pandas, modin, dask or dask_modin only $ pip install \"virtual_dataframe[$VDF_MODE]\" The core of the framework can be installed with $ pip install \"virtual_dataframe\" Installing from the GitHub master branch $ pip install \"virtual_dataframe[all]@git+https://github.com/pprados/virtual-dataframe\" or, for a selected framework $ VDF_MODE=... # pandas, modin, dask or dask_modin only $ pip install \"virtual_dataframe[$VDF_MODE]@git+https://github.com/pprados/virtual-dataframe\" The core of the framework can be installed with $ pip install \"virtual_dataframe@git+https://github.com/pprados/virtual-dataframe\" API api comments vdf.@delayed Delayed function (do nothing or dask.delayed) vdf.concat(...) Merge VDataFrame vdf.read_csv(...) Read VDataFrame from CSVs glob files vdf.from_pandas(pdf, npartitions=...) Create Virtual Dataframe from Pandas DataFrame vdf.from_backend(vdf, npartitions=...) Create Virtual Dataframe from backend dataframe vdf.compute([...]) Compute multiple @delayed functions VDataFrame(data, npartitions=...) Create DataFrame in memory (only for test) VSeries(data, npartitions=...) Create Series in memory (only for test) BackEndDataFrame The class of dask/ray backend dataframe BackEndSeries The class of dask/ray backend series BackEnd The backend framework VDataFrame.compute() Compute the virtual dataframe VDataFrame.visualize() Create an image with the graph VDataFrame.to_pandas() Convert to pandas dataframe VDataFrame.to_csv() Save to glob files VDataFrame.to_numpy() Convert to numpy array VDataFrame.to_backend() Convert to backend dataframe VDataFrame.categorize() Detect all categories VDataFrame.apply_rows() Apply rows, GPU template VDataFrame.map_partitions() Apply function for each parttions VSeries.compute() Compute the virtual series VSeries.to_pandas() Convert to pandas series VSeries.to_backend() Convert to backend series VSeries.to_numpy() Convert to numpy array VClient(...) The connexion with the cluster VDF_MODE The current mode Mode The enumeration of differents mode You can read a sample notebook here for an exemple of all API. Each API propose a specific version for each framework. For example: - the toPandas() with Panda, return self - @delayed use the dask @delayed or do nothing, and apply the code when the function was called. In the first case, the function return a part of the graph. In the second case, the function return immediately the result. - read_csv(\"file*.csv\") read each file in parallele with dask by each worker, and read sequencially each file, and combine each dataframe, with a framework without distribution (pandas, cudf) - save_csv(\"file*.csv\") write each file in parallele with dask, and write one file \"file.csv\" (without the star) with a framework without distribution (pandas, cudf) - ... Compatibility This project is just a wrapper. So, it inherits limitations and bugs from other projects. Sorry for that. Limitations pandas All data must be in DRAM modin Read this cudf All data must be in VRAM All data types in cuDF are nullable Iterating over a cuDF Series, DataFrame or Index is not supported. Join (or merge) and groupby operations in cuDF do not guarantee output ordering. The order of operations is not always deterministic Cudf does not support duplicate column names Cudf also supports .apply() it relies on Numba to JIT compile the UDF and execute it on the GPU .apply(result_type=...) not supported dask transpose() and MultiIndex are not implemented Column assignment doesn't support type list dask-cudf See cudf and dask. Categories with strings not implemented To be compatible with all framework, you must only use the common features. small data middle data big data 1-CPU pandas Limit:+ n-CPU modin Limit+ dask or dask_modin Limit:++ GPU cudf Limit:++ dask_cudf Limit:+++ To develop, you can choose the level to be compatible with others frameworks. Each cell is strongly compatible with the upper left part. No need of GPU? If you don't need to use a GPU, then develop for dask and use mode in bold . small data middle data big data 1-CPU pandas Limit:+ n-CPU modin Limite+ dask or dask_modin Limit:++ GPU cudf Limit:++ dask_cudf Limit:+++ You can ignore this API: - VDataFrame.apply_rows() No need of big data? If you don't need to use big data, then develop for cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limit:+ n-CPU modin Limit+ dask or dask_modin Limit:++ GPU cudf Limit:++ dask_cudf Limit:+++ You can ignore these API: - @delayed - map_partitions() - categorize() - compute() - npartitions=... Need all possibility? To be compatible with all mode, develop for dask_cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limit:+ n-CPU modin Limit+ dask or dask_modin Limit:++ GPU cudf Limit:++ dask_cudf Limit:+++ and accept all the limitations. Best practices For write a code, optimized with all frameworks, you must use some best practices .\" Use read file It's not a good idea to use the constructor of VDataFrame or VSeries because, all the datas must be in memory in the driver. If the dataframe is big, an out of memory can happen. To partitionning the job, create multiple files, and use read_csv(\"filename*.csv\") or other file format. Then, each worker can read directly a partition. The constructor for VDataFrame and VSeries are present, only to help to write some unit test, but may not be used in production. No loop It's not a good idea to iterate over row of DataFrame. Some framework are very slow with this approach. It's better to use apply() or apply_rows() to distribute the code in the cluster and GPU. The code used in apply , will be compiled to CPU or GPU, before using, with some frameworks. FAQ The code run with dask, but not with modin, pandas or cudf ? You must use only the similare functionality, and only a subpart of Pandas. Develop for dask_cudf . it's easier to be compatible with others frameworks. .compute() is not defined with pandas, cudf ? If your @delayed function return something, other than a VDataFrame or VSerie , the objet has not the method .compute() . You can solve this, with: @delayed def f()-> int: return 42 real_result,=compute(f()) # Warning, compute return a tuple. The comma is important. a,b = compute(f(),f()) Contribute The latest version Clone the git repository $ git clone --recurse-submodules https://github.com/pprados/virtual-dataframe.git Installation Go inside the directory and $ make configure $ conda activate virtual_dataframe $ make test Tests To test the project $ make test To validate the typing $ make typing To validate all the project $ make validate Project Organization \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u251c\u2500\u2500 conda_recipe <- Script to build the conda package \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 \u2502 `1.0-jqp-initial-data-exploration`. \u2502 \u2514\u2500\u2500 demo.ipynb <- A demo for all API \u2502 \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .[tests]) \u2502 so sources can be imported and dependencies installed \u251c\u2500\u2500 virtual_dataframe <- Source code for use in this project \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2514\u2500\u2500 *.py <- Framework codes \u2502 \u2514\u2500\u2500 tests <- Unit and integrations tests ((Mark directory as a sources root).","title":"Home"},{"location":"#virtual-dataframe","text":"Full documentation","title":"Virtual DataFrame"},{"location":"#motivation","text":"With Panda like dataframe, do you want to create a code, and choose at the end, the framework to use? Do you want to be able to choose the best framework after simply performing performance measurements? This framework unifies multiple Panda-compatible components, to allow the writing of a single code, compatible with all.","title":"Motivation"},{"location":"#synopsis","text":"With some parameters and Virtual classes, it's possible to write a code, and execute this code: - With or without multicore - With or without cluster (multi nodes) - With or without GPU To do that, we create some virtual classes, add some methods in others classes, etc. It's difficult to use a combinaison of framework, with the same classe name, with similare semantic, etc. For example, if you want to use in the same program, Dask, cudf, pandas and modin, you must manage: - pandas.DataFrame , pandas,Series - modin.pandas.DataFrame , modin.pandas.Series - cudf.DataFrame , cudf.Series - dask.DataFrame , dask.Series With cudf , the code must call .to_pandas() . With dask, the code must call .compute() , can use @delayed or dask.distributed.Client . etc. We propose to replace all these classes and scenarios, with a uniform model , inspired by dask (the more complex API). Then, it is possible to write one code, and use it in differents environnements and frameworks. This project is essentially a back-port of Dask+Cudf to others frameworks. To reduce the confusion, you must use the classes VDataFrame and VSeries (The prefix V is for Virtual ). These classes propose the methods .to_pandas() and .compute() for each version, but are the real classes of the selected framework. A new @delayed annotation can be use, with or without Dask. With some parameters, the real classes may be pandas.DataFrame , modin.pandas.DataFrame , cudf.DataFrame , dask.dataframe.DataFrame with Pandas or dask.dataframe.DataFrame with cudf (with Pandas or cudf for each partition). To manage the initialisation of a Dask or Ray cluster, you must use the VClient() . This alias, can be automatically initialized with some environment variables. # Sample of code, compatible Pandas, cudf, modin, dask and dask_cudf from virtual_dataframe import * TestDF = VDataFrame with (VClient()): @delayed def my_function(data: TestDF) -> TestDF: return data rc = my_function(VDataFrame({\"data\": [1, 2]}, npartitions=2)) print(rc.to_pandas()) With this framework, you can select your environment, to run or debug your code. env Environement VDF_MODE=pandas Only Python with classical pandas VDF_MODE=modin Python with local modin VDF_MODE=cudf Python with local cuDF (GPU) VDF_MODE=dask Dask with local multiple process and pandas VDF_MODE=dask-cudf Dask with local multiple process and cuDF VDF_MODE=dask DEBUG=True Dask with single thread and pandas VDF_MODE=dask-cudf DEBUG=True Dask with single thread and cuDF VDF_MODE=dask VDF_CLUSTER=dask://localhost Dask with local cluster and pandas VDF_MODE=dask-cudf VDF_CLUSTER=dask://localhost Dask with local cuda cluster and cuDF VDF_MODE=dask VDF_CLUSTER=dask://...:ppp Dask with remote cluster and Pandas VDF_MODE=dask-cudf VDF_CLUSTER=dask://...:ppp Dask with remote cluster and cuDF VDF_MODE=dask-modin VDF_CLUSTER=dask://localhost Dask with local cluster and modin VDF_MODE=ray-modin VDF_CLUSTER=ray://localhost Ray with local cluster and modin VDF_MODE=dask-modin VDF_CLUSTER=dask://...:ppp Dask with remote cluster and modin VDF_MODE=ray-modin VDF_CLUSTER=ray://...:ppp Ray with remote cluster and modin The real compatibilty between the differents simulation of Pandas, depends on the implement of the modin, cudf or dask. Sometime, you can use the VDF_MODE variable, to update some part of code, between the selected backend. It's not always easy to write a code compatible with all scenario, but it's possible. Generally, add just .compute() and/or .to_pandas() at the end of the ETL, is enough. But, you must use, only the common feature with all frameworks. After this effort, it's possible to compare the performance about the differents technologies, or propose a component, compatible with differents scenario. For the deployment of your project, you can select the best framework for your process (in a dockerfile?), with only one ou two environment variable.","title":"Synopsis"},{"location":"#cluster","text":"To connect to a cluster, use VDF_CLUSTER with protocol, host and optionaly, the port. - dask://locahost:8787 - ray://locahost:10001 - ray:auto - or alternativelly, use DASK_SCHEDULER_SERVICE_HOST and DASK_SCHEDULER_SERVICE_PORT VDF_MODE DEBUG VDF_CLUSTER Scheduler pandas - - No scheduler cudf - - No scheduler modin - - No scheduler dask Yes - synchronous dask No - processes dask No dask://localhost LocalCluster dask_modin No - LocalCluster dask_modin No dask://localhost LocalCluster dask_modin No dask:// Dask cluster ray_modin No ray:auto Dask cluster ray_modin No ray://localhost Dask cluster ray_modin No ray:// Dask cluster dask-cudf No dask://localhost LocalCUDACluster dask-cudf No dask:// Dask cluster Sample: from virtual_dataframe import VClient with (VClient()) # Now, use the scheduler","title":"Cluster"},{"location":"#installation","text":"Warning: At this time, the packages are not published in pip or conda repositories","title":"Installation"},{"location":"#installing-with-conda-recommended","text":"$ conda install -q -y \\ -c rapidsai -c nvidia -c conda-forge \\ \"virtual_dataframe-all\" or, for only one mode: $ VDF_MODE=... $ conda install -q -y \\ -c rapidsai -c nvidia -c conda-forge \\ virtual_dataframe-$VDF_MODE The package virtual_dataframe (without suffix) has no framework dependencies.","title":"Installing with Conda (recommended)"},{"location":"#installing-with-pip","text":"With PIP, it's not possible to install NVidia Rapids to use the GPU. A limited list of dependencies is possible. $ pip install \"virtual_dataframe[all]\" or, for a selected framework $ VDF_MODE=... # pandas, modin, dask or dask_modin only $ pip install \"virtual_dataframe[$VDF_MODE]\" The core of the framework can be installed with $ pip install \"virtual_dataframe\"","title":"Installing with pip"},{"location":"#installing-from-the-github-master-branch","text":"$ pip install \"virtual_dataframe[all]@git+https://github.com/pprados/virtual-dataframe\" or, for a selected framework $ VDF_MODE=... # pandas, modin, dask or dask_modin only $ pip install \"virtual_dataframe[$VDF_MODE]@git+https://github.com/pprados/virtual-dataframe\" The core of the framework can be installed with $ pip install \"virtual_dataframe@git+https://github.com/pprados/virtual-dataframe\"","title":"Installing from the GitHub master branch"},{"location":"#api","text":"api comments vdf.@delayed Delayed function (do nothing or dask.delayed) vdf.concat(...) Merge VDataFrame vdf.read_csv(...) Read VDataFrame from CSVs glob files vdf.from_pandas(pdf, npartitions=...) Create Virtual Dataframe from Pandas DataFrame vdf.from_backend(vdf, npartitions=...) Create Virtual Dataframe from backend dataframe vdf.compute([...]) Compute multiple @delayed functions VDataFrame(data, npartitions=...) Create DataFrame in memory (only for test) VSeries(data, npartitions=...) Create Series in memory (only for test) BackEndDataFrame The class of dask/ray backend dataframe BackEndSeries The class of dask/ray backend series BackEnd The backend framework VDataFrame.compute() Compute the virtual dataframe VDataFrame.visualize() Create an image with the graph VDataFrame.to_pandas() Convert to pandas dataframe VDataFrame.to_csv() Save to glob files VDataFrame.to_numpy() Convert to numpy array VDataFrame.to_backend() Convert to backend dataframe VDataFrame.categorize() Detect all categories VDataFrame.apply_rows() Apply rows, GPU template VDataFrame.map_partitions() Apply function for each parttions VSeries.compute() Compute the virtual series VSeries.to_pandas() Convert to pandas series VSeries.to_backend() Convert to backend series VSeries.to_numpy() Convert to numpy array VClient(...) The connexion with the cluster VDF_MODE The current mode Mode The enumeration of differents mode You can read a sample notebook here for an exemple of all API. Each API propose a specific version for each framework. For example: - the toPandas() with Panda, return self - @delayed use the dask @delayed or do nothing, and apply the code when the function was called. In the first case, the function return a part of the graph. In the second case, the function return immediately the result. - read_csv(\"file*.csv\") read each file in parallele with dask by each worker, and read sequencially each file, and combine each dataframe, with a framework without distribution (pandas, cudf) - save_csv(\"file*.csv\") write each file in parallele with dask, and write one file \"file.csv\" (without the star) with a framework without distribution (pandas, cudf) - ...","title":"API"},{"location":"#compatibility","text":"This project is just a wrapper. So, it inherits limitations and bugs from other projects. Sorry for that. Limitations pandas All data must be in DRAM modin Read this cudf All data must be in VRAM All data types in cuDF are nullable Iterating over a cuDF Series, DataFrame or Index is not supported. Join (or merge) and groupby operations in cuDF do not guarantee output ordering. The order of operations is not always deterministic Cudf does not support duplicate column names Cudf also supports .apply() it relies on Numba to JIT compile the UDF and execute it on the GPU .apply(result_type=...) not supported dask transpose() and MultiIndex are not implemented Column assignment doesn't support type list dask-cudf See cudf and dask. Categories with strings not implemented To be compatible with all framework, you must only use the common features. small data middle data big data 1-CPU pandas Limit:+ n-CPU modin Limit+ dask or dask_modin Limit:++ GPU cudf Limit:++ dask_cudf Limit:+++ To develop, you can choose the level to be compatible with others frameworks. Each cell is strongly compatible with the upper left part.","title":"Compatibility"},{"location":"#no-need-of-gpu","text":"If you don't need to use a GPU, then develop for dask and use mode in bold . small data middle data big data 1-CPU pandas Limit:+ n-CPU modin Limite+ dask or dask_modin Limit:++ GPU cudf Limit:++ dask_cudf Limit:+++ You can ignore this API: - VDataFrame.apply_rows()","title":"No need of GPU?"},{"location":"#no-need-of-big-data","text":"If you don't need to use big data, then develop for cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limit:+ n-CPU modin Limit+ dask or dask_modin Limit:++ GPU cudf Limit:++ dask_cudf Limit:+++ You can ignore these API: - @delayed - map_partitions() - categorize() - compute() - npartitions=...","title":"No need of big data?"},{"location":"#need-all-possibility","text":"To be compatible with all mode, develop for dask_cudf and use mode in bold .. small data middle data big data 1-CPU pandas Limit:+ n-CPU modin Limit+ dask or dask_modin Limit:++ GPU cudf Limit:++ dask_cudf Limit:+++ and accept all the limitations.","title":"Need all possibility?"},{"location":"#best-practices","text":"For write a code, optimized with all frameworks, you must use some best practices .\"","title":"Best practices"},{"location":"#use-read-file","text":"It's not a good idea to use the constructor of VDataFrame or VSeries because, all the datas must be in memory in the driver. If the dataframe is big, an out of memory can happen. To partitionning the job, create multiple files, and use read_csv(\"filename*.csv\") or other file format. Then, each worker can read directly a partition. The constructor for VDataFrame and VSeries are present, only to help to write some unit test, but may not be used in production.","title":"Use read file"},{"location":"#no-loop","text":"It's not a good idea to iterate over row of DataFrame. Some framework are very slow with this approach. It's better to use apply() or apply_rows() to distribute the code in the cluster and GPU. The code used in apply , will be compiled to CPU or GPU, before using, with some frameworks.","title":"No loop"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#the-code-run-with-dask-but-not-with-modin-pandas-or-cudf","text":"You must use only the similare functionality, and only a subpart of Pandas. Develop for dask_cudf . it's easier to be compatible with others frameworks.","title":"The code run with dask, but not with modin, pandas or cudf ?"},{"location":"#compute-is-not-defined-with-pandas-cudf","text":"If your @delayed function return something, other than a VDataFrame or VSerie , the objet has not the method .compute() . You can solve this, with: @delayed def f()-> int: return 42 real_result,=compute(f()) # Warning, compute return a tuple. The comma is important. a,b = compute(f(),f())","title":".compute() is not defined with pandas, cudf ?"},{"location":"#contribute","text":"","title":"Contribute"},{"location":"#the-latest-version","text":"Clone the git repository $ git clone --recurse-submodules https://github.com/pprados/virtual-dataframe.git","title":"The latest version"},{"location":"#installation_1","text":"Go inside the directory and $ make configure $ conda activate virtual_dataframe $ make test","title":"Installation"},{"location":"#tests","text":"To test the project $ make test To validate the typing $ make typing To validate all the project $ make validate","title":"Tests"},{"location":"#project-organization","text":"\u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u251c\u2500\u2500 conda_recipe <- Script to build the conda package \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 \u2502 `1.0-jqp-initial-data-exploration`. \u2502 \u2514\u2500\u2500 demo.ipynb <- A demo for all API \u2502 \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .[tests]) \u2502 so sources can be imported and dependencies installed \u251c\u2500\u2500 virtual_dataframe <- Source code for use in this project \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2514\u2500\u2500 *.py <- Framework codes \u2502 \u2514\u2500\u2500 tests <- Unit and integrations tests ((Mark directory as a sources root).","title":"Project Organization"},{"location":"CHANGELOG/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] Added Pandas, cudf, modin, dask, dask_cudf and dask_modin frameworks Changed Deprecated Removed Fixed Security [0.0.1] - YYYY-MM-DD TODO: Update CHANGELOG.md","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"[Unreleased]"},{"location":"CHANGELOG/#added","text":"Pandas, cudf, modin, dask, dask_cudf and dask_modin frameworks","title":"Added"},{"location":"CHANGELOG/#changed","text":"","title":"Changed"},{"location":"CHANGELOG/#deprecated","text":"","title":"Deprecated"},{"location":"CHANGELOG/#removed","text":"","title":"Removed"},{"location":"CHANGELOG/#fixed","text":"","title":"Fixed"},{"location":"CHANGELOG/#security","text":"","title":"Security"},{"location":"CHANGELOG/#001-yyyy-mm-dd","text":"TODO: Update CHANGELOG.md","title":"[0.0.1] - YYYY-MM-DD"},{"location":"commands/","text":"Make commands The Makefile contains the central entry points for common tasks related to this project. Commands make help will print all majors target make configure will prepare the environment (conda venv, kernel, ...) make run-% will invoke all script in lexical order from scripts/<% dir> make lint will lint the code make test will run all tests make typing will check the typing make validate will validate the version before commit make clean will clean current environment make docs will create and show a HTML documentation in 'build/' make dist will create a full wheel distribution Jupyter commands make notebook will start a jupyter notebook make remove-kernel will remove the project's kernel make nb-run-% will execute all notebooks make nb-convert will convert all notebooks to python make clean-notebooks will clean all datas in the notebooks Twine commands make check-twine will check the packaging before publication make test-twine will publish the package in test.pypi.org <https://test.pypi.org> _) make twine will publish the package in pypi.org <https://pypi.org> _) Conda commands make conda-build will build the conda package make conda-debug will build the package in debug mode make conda-convert will convert the package for all platform make conda-install will install the conda package make conda-purge will remove the build artefact","title":"Commands"},{"location":"commands/#make-commands","text":"The Makefile contains the central entry points for common tasks related to this project.","title":"Make commands"},{"location":"commands/#commands","text":"make help will print all majors target make configure will prepare the environment (conda venv, kernel, ...) make run-% will invoke all script in lexical order from scripts/<% dir> make lint will lint the code make test will run all tests make typing will check the typing make validate will validate the version before commit make clean will clean current environment make docs will create and show a HTML documentation in 'build/' make dist will create a full wheel distribution","title":"Commands"},{"location":"commands/#jupyter-commands","text":"make notebook will start a jupyter notebook make remove-kernel will remove the project's kernel make nb-run-% will execute all notebooks make nb-convert will convert all notebooks to python make clean-notebooks will clean all datas in the notebooks","title":"Jupyter commands"},{"location":"commands/#twine-commands","text":"make check-twine will check the packaging before publication make test-twine will publish the package in test.pypi.org <https://test.pypi.org> _) make twine will publish the package in pypi.org <https://pypi.org> _)","title":"Twine commands"},{"location":"commands/#conda-commands","text":"make conda-build will build the conda package make conda-debug will build the package in debug mode make conda-convert will convert the package for all platform make conda-install will install the conda package make conda-purge will remove the build artefact","title":"Conda commands"},{"location":"contribute/","text":"Contribute R\u00e9cup\u00e9rez les sources $ git clone https://github.com/pprados/virtual_dataframe virtual_dataframe puis: $ cd virtual_dataframe $ make configure $ conda activate virtual_dataframe $ make test Principes d'organisation du projet Un make validate est ex\u00e9cuter automatiquement avant un git push sur la branche master . Il ne doit pas g\u00e9n\u00e9rer d'erreurs. Cela permet d'avoir un build incassable, en ne publiant que des commits valid\u00e9s. Il est possible de forcer tout de m\u00eame le push avec FORCE=y git push . Cela permet d'avoir l'\u00e9quivalent d'une CI/CD en local. Bien entendu, cela peut etre supprim\u00e9 si une plateforme d'int\u00e9gration est disponible. La version du projet et d\u00e9duite du tag courant GIT Le code du projet doit \u00eatre pr\u00e9sent dans le package virtual_dataframe . Ils doivent \u00eatre ex\u00e9cut\u00e9 depuis le home du projet. Pour cela, une astuce consiste \u00e0 forcer le home dans les templates Python de PyCharm. Ainsi, un run depuis un source fonctionne directement, sans manipulation. Le typing <https://realpython.com/python-type-checking/> _ est recommand\u00e9, afin d'am\u00e9liorer la qualit\u00e9 du code et sa documentation. Vous pouvez v\u00e9rifier cela avec make typing , ou ajouter automatiquement une partie du typing \u00e0 votre code avec make add-typing . Truc et astuces Quelques astuces disponibles dans le projet. Les test Les tests sont divis\u00e9s en deux groupes : unit-test et functional-test . Il est possible d'ex\u00e9cuter l'un des groups \u00e0 la fois ( make (unit|functional)-test ) ou l'ensemble ( make test ). Les tests sont parall\u00e9lis\u00e9s lors de leurs executions. Cela permet de b\u00e9n\u00e9ficier des architectures avec plusieurs coeurs CPU. Pour d\u00e9sactiver temporairement cette fonctionnalit\u00e9, il suffit d'indiquer un nombre de coeur r\u00e9duit \u00e0 utiliser. Par exemple : NPROC=1 make test V\u00e9rifier le build Pour v\u00e9rifier que le Makefile est correct, vous pouvez vider l'environement conda avec make clean-venv puis lancer votre r\u00e8gle. Elle doit fonctionner directement et doit m\u00eame pouvoir \u00eatre ex\u00e9cut\u00e9 deux fois de suite, sans rejouer le traitement deux fois. Par exemple : $ make validate D\u00e9verminer le Makefile Il est possible de connaitre la valeur calcul\u00e9e d'une variable dans le Makefile . Pour cela, utilisez make dump-MA_VARIABLE . Pour comprendre les r\u00e8gles de d\u00e9pendances justifiant un build, utilisez make --debug -n . {% if cookiecutter.use_jupyter == 'y' %} Convertir un notebook Il est possible de convertir un notebook en script, puis de lui ajouter un typage. make nb-convert add-typing Gestion des r\u00e8gles ne produisant pas de fichiers Le code g\u00e9n\u00e8re des fichiers .make-<rule> pour les r\u00e8gles ne produisant pas de fichier, comme test ou validate par exemple. Vous pouvez ajoutez ces fichiers \u00e0 GIT pour m\u00e9moriser la date de la derni\u00e8re ex\u00e9cution. Ainsi, les autres d\u00e9veloppeurs n'ont pas besoin de les r\u00e9-executer si ce n'est pas n\u00e9cessaire. {% endif %} Recommandations Utilisez un CHANGELOG bas\u00e9 sur Keep a Changelog <https://keepachangelog.com/en/1.0.0/> _, Utilisez un format de version conforme \u00e0 Semantic Versioning <https://semver.org/spec/v2.0.0.html> _. Utiliser une approche Develop/master branch <https://nvie.com/posts/a-successful-git-branching-model/> _. Faite toujours un make validate avant de commiter le code","title":"contribute"},{"location":"contribute/#contribute","text":"R\u00e9cup\u00e9rez les sources $ git clone https://github.com/pprados/virtual_dataframe virtual_dataframe puis: $ cd virtual_dataframe $ make configure $ conda activate virtual_dataframe $ make test","title":"Contribute"},{"location":"contribute/#principes-dorganisation-du-projet","text":"Un make validate est ex\u00e9cuter automatiquement avant un git push sur la branche master . Il ne doit pas g\u00e9n\u00e9rer d'erreurs. Cela permet d'avoir un build incassable, en ne publiant que des commits valid\u00e9s. Il est possible de forcer tout de m\u00eame le push avec FORCE=y git push . Cela permet d'avoir l'\u00e9quivalent d'une CI/CD en local. Bien entendu, cela peut etre supprim\u00e9 si une plateforme d'int\u00e9gration est disponible. La version du projet et d\u00e9duite du tag courant GIT Le code du projet doit \u00eatre pr\u00e9sent dans le package virtual_dataframe . Ils doivent \u00eatre ex\u00e9cut\u00e9 depuis le home du projet. Pour cela, une astuce consiste \u00e0 forcer le home dans les templates Python de PyCharm. Ainsi, un run depuis un source fonctionne directement, sans manipulation. Le typing <https://realpython.com/python-type-checking/> _ est recommand\u00e9, afin d'am\u00e9liorer la qualit\u00e9 du code et sa documentation. Vous pouvez v\u00e9rifier cela avec make typing , ou ajouter automatiquement une partie du typing \u00e0 votre code avec make add-typing .","title":"Principes d'organisation du projet"},{"location":"contribute/#truc-et-astuces","text":"Quelques astuces disponibles dans le projet.","title":"Truc et astuces"},{"location":"contribute/#les-test","text":"Les tests sont divis\u00e9s en deux groupes : unit-test et functional-test . Il est possible d'ex\u00e9cuter l'un des groups \u00e0 la fois ( make (unit|functional)-test ) ou l'ensemble ( make test ). Les tests sont parall\u00e9lis\u00e9s lors de leurs executions. Cela permet de b\u00e9n\u00e9ficier des architectures avec plusieurs coeurs CPU. Pour d\u00e9sactiver temporairement cette fonctionnalit\u00e9, il suffit d'indiquer un nombre de coeur r\u00e9duit \u00e0 utiliser. Par exemple : NPROC=1 make test","title":"Les test"},{"location":"contribute/#verifier-le-build","text":"Pour v\u00e9rifier que le Makefile est correct, vous pouvez vider l'environement conda avec make clean-venv puis lancer votre r\u00e8gle. Elle doit fonctionner directement et doit m\u00eame pouvoir \u00eatre ex\u00e9cut\u00e9 deux fois de suite, sans rejouer le traitement deux fois. Par exemple : $ make validate","title":"V\u00e9rifier le build"},{"location":"contribute/#deverminer-le-makefile","text":"Il est possible de connaitre la valeur calcul\u00e9e d'une variable dans le Makefile . Pour cela, utilisez make dump-MA_VARIABLE . Pour comprendre les r\u00e8gles de d\u00e9pendances justifiant un build, utilisez make --debug -n . {% if cookiecutter.use_jupyter == 'y' %}","title":"D\u00e9verminer le Makefile"},{"location":"contribute/#convertir-un-notebook","text":"Il est possible de convertir un notebook en script, puis de lui ajouter un typage. make nb-convert add-typing","title":"Convertir un notebook"},{"location":"contribute/#gestion-des-regles-ne-produisant-pas-de-fichiers","text":"Le code g\u00e9n\u00e8re des fichiers .make-<rule> pour les r\u00e8gles ne produisant pas de fichier, comme test ou validate par exemple. Vous pouvez ajoutez ces fichiers \u00e0 GIT pour m\u00e9moriser la date de la derni\u00e8re ex\u00e9cution. Ainsi, les autres d\u00e9veloppeurs n'ont pas besoin de les r\u00e9-executer si ce n'est pas n\u00e9cessaire. {% endif %}","title":"Gestion des r\u00e8gles ne produisant pas de fichiers"},{"location":"contribute/#recommandations","text":"Utilisez un CHANGELOG bas\u00e9 sur Keep a Changelog <https://keepachangelog.com/en/1.0.0/> _, Utilisez un format de version conforme \u00e0 Semantic Versioning <https://semver.org/spec/v2.0.0.html> _. Utiliser une approche Develop/master branch <https://nvie.com/posts/a-successful-git-branching-model/> _. Faite toujours un make validate avant de commiter le code","title":"Recommandations"}]}